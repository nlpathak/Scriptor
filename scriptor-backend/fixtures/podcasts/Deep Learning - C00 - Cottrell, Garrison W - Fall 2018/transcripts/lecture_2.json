{
    "Blurbs": {
        "I've told you three times now if you should be off in that situation, so what do I write here? How about here? How about here? Okay, the bias is the opposite of threshold. So Stevens one. That was one one one. That was the threshold down 2-1 1-1. And now if I give it one one. Wow, a generalized has something in her son before and it will be ": [
            1014.9,
            1053.8,
            25
        ],
        "Listen to a podcast. Okay, if you're just joining us. I now have a microphone. Okay, and now is 0110 * 101 * 1 is one one is greater than or equal to one. So it's on and the rest are like that. Okay. How about a set of Weights in the threshold to do hand? Yeah. 1 1 2 Yeah, so now this won't fire until we get both ": [
            1.9,
            108.5,
            0
        ],
        "So I'll put his lunch should be zero I get a minus one. So how there should be an X1 here? Oh, yeah, this is assuming this input is 1 What affects is inactive I-80 then the weight doesn't change. Okay. and Oh wait, that's that's an old slide. I should take that side out. What side is that? 27 remove slide 27. What about the bias Retreat? W0 is ": [
            1470.2,
            1517.2,
            39
        ],
        "So that the next time I won't be on in that situation and here I stay active inputs. I mean that the input is R1 if the output of zero and it should be big. Should be one then raise the waist active inputs including the bias. so this is a very simple learning roll. So we're going to choose or and we're going to stop here for a quick ": [
            595.0,
            625.3,
            15
        ],
        "So w t x x a minus x p which is all segments of the line Y equals 0 so their inner product is zero. So there perpendicular. So the weight the weight factor determines where the line is or if it's a plane where the plane is. and then The distance to the origin that is where you set your threshold kind of thing is El people in that ": [
            1895.7,
            1933.0,
            50
        ],
        "There's a story we didn't I don't know if it's apocryphal or not. But basically rosenblatt, of course the Army took pictures of Tanks. Some of the pictures had tanks in them. Some of them didn't you wanted to perceptron to tell you where there was a tank in the picture and it did that great. The problem was it turned out all the pictures with tanks were taken in ": [
            356.2,
            387.2,
            8
        ],
        "This is fall 17. So it's probably not going to change very much. We're going to talk about logistic regression and multinomial regression. And the basic idea is that instead of the output being this. Bang bang kind of things. There's going to be some nonlinear function here. So instead of Instead of a function that looks like this. We're going to have a function that looks like this. And ": [
            2713.3,
            2755.6,
            69
        ],
        "a Gary detector? So I put in an image, which is just a table of numbers. And you know, there's me. And I have a wait for every pixel. That's this is what you guys are actually doing here and I Every time now one thing it's too late. Well. You can kind of see what I did hear is I just added and subtracted the input vector. To the ": [
            1123.1,
            1163.3,
            28
        ],
        "a perceptron has and I haven't told you what that guarantee is yet, but basically leave it mostly as an exercise for the reader, but So here's this and here's the X1 and X2. If I can find weights and a threshold then I'll make that fire 400 which I can then I can it's supposed to be off. I can never mind. This one here 400 South got a ": [
            246.5,
            285.0,
            5
        ],
        "a weight from a unit that's always a constant one. So the Opera is one should be zero lower the weights and lower the bias. This leads to subtracting one from the bias or subtracting learning right from the bias. What if we get it right? Well if we get it right then TN, why are the same so this is zero and nothing happens. Okay. So one line of ": [
            1517.2,
            1550.9,
            40
        ],
        "again. We're WW1 through WD. And you have to prove this in your homework. So here's one kind of proof. Do you have to prove this to its the length minus? W0 over the length of w? And in doing this you have to assume that the weight factor is pointing that way because otherwise he got a negative distance, but the answer is - W 0 / the length ": [
            1933.0,
            1965.3,
            51
        ],
        "and image space and image. Remember we said an image is not 2D an image of the point in a high dimensional space where every Dimension corresponds to a pixel you're finding some separating hyperplane in that space that puts the happy guys on one side and the sad guys on the other. Okay. and I'm going to abuse our notation don't report me, but I'm going to make W ": [
            1786.4,
            1826.0,
            47
        ],
        "and what about that point? Yeah why I am. So this is think of this was input space. And it's 2D in this case. And at this point why I is bigger than YJ. NYK and it turns out that every region is condex meaning. That if any two points are in this region, then the point between them is in this region. And so it look it really does ": [
            2188.9,
            2234.7,
            58
        ],
        "because the guys near the boundaries are changing the weights too much by may make the learning right a little smaller. It slows down. I make it a little smaller. It's smoother. And I can even have the learning rate be that small and it's still. Still gets there eventually. But you know, but you don't want your learning rate to be too big to start. So what if we ": [
            2583.6,
            2621.4,
            66
        ],
        "blue dots where you can do in cases like that. There's something called the pocket algorithm where he keeps the best weight so far in your pocket, or you can just make the learning rate smaller and smaller until it stops and I'll still make mistakes but at least So that's the that's the separating plane. And it's going to keep doing this as long as I do this until ": [
            2550.2,
            2583.6,
            65
        ],
        "bright sunlight in the ones with outward and it was just figuring out how bright the image was. Didn't didn't generalize very well to dim pictures with tanks in them or bright pictures without tanks. So rosenblatt discovered this learning rule called the perceptron convergence procedure and it's called that because it's stop straining after a while. It's guaranteed to learn anything computable. But to some people would call this ": [
            387.2,
            421.3,
            9
        ],
        "by the same road all the ways, but I should say I can say here I might as well that it doesn't matter. This doesn't need to be 0 or 1 it can be any real number. And then how big it is makes a difference and how much it's going to change the weights. so, you know thinking about this is just doing Boolean functions is kind of Not ": [
            1399.1,
            1429.5,
            37
        ],
        "code to rule them all. one line Divine them in the Darkness Okay. Okay, and X does not need to be binary in that case it changes the weights in proportion to its size. OKC demo, and the demo is not going to show you the demo just yet. Okay, still need that at these sides a little bit. I made a bunch of changes today. Okay computer and learn ": [
            1550.9,
            1592.0,
            41
        ],
        "demo on the board because he have to do this in your homework. So Deus Ex 1 years, that's 2 years that or of those two. 0 0 0 0 1 1 1 0 1 1 1 1 Okay, and now I'm going to make a little table x 1 x 2. Out why the output tees at Target? W0 W1 W2 And I'm going to train this thing in ": [
            625.3,
            678.9,
            16
        ],
        "do excellent cuz he's supposed to be on here on here off here and off here. And there is no way to put a line down here that separates this in this from this and this everybody always kind of goes like that for a while, but So that's why it can't that's another way of showing why it can't do X or so when you're doing happy vs. Sad ": [
            1755.0,
            1786.4,
            46
        ],
        "equal to that and then there's some has to be less than that not possible. Okay, so you can't compute excellent Mexican passport said now if you had hidden units you could compute any Boolean function, but there is no learning rule for networks with hidden units and we don't think one will ever be discovered. and by that they meant anyone that has the same kind of guarantee that ": [
            213.2,
            246.5,
            4
        ],
        "even do it in class time. So if I needed to do the first row, that means the W 1 * 0 + W 2 * 0 would have to be less than the threshold. So that means the threshold is positive. How to get the second row I need W 1 * 0 + W 2 * 1 is greater than or equal to the threshold. That means W2 ": [
            145.9,
            180.5,
            2
        ],
        "example of pick this one again. And give it to zero Zero's input. What's the output? 1 It's supposed to be zero. So 0 * 000 * 1 is zero a 1000 is greater than equal to zero. That's one teacher says it's still your turn. Okay, so we were on and we should be the next time it's e00. I'll be okay these word active inputs. So they stay ": [
            878.4,
            925.4,
            22
        ],
        "exponential number of hidden units in so that's not very that's it. It's a good construction, but it's not very practical turns out you can do parody with you no end bit parity with end hidden units are linear number. So that's what perceptrons are and again the goal was to make this new really inspired machine that could categorize inputs, but learn to do it from examples. And so ": [
            319.6,
            354.9,
            7
        ],
        "going to randomly pick an example of pick this one and give it to Euro 02. What's the output? Okay, this is to raise your acetylcholine. It's one zero is greater than equal to zero. Okay, so the output is 1 the target is zero. And so the system says it's still your turn. Okay, so we were And we should be off so we want to lower the weights ": [
            723.4,
            760.2,
            18
        ],
        "got 14 minutes left. Let me just give up a little demo. I guess not this demo. Let's see. What am I looking for desktop? Okay. Quit work what happened? There should be some other stuff, too. Okay. so here's a little demo of a perceptron like thing that's interesting and let's try again. Why is it not doing now? 720 area code 6 Okay, so this is not when ": [
            2347.4,
            2438.0,
            62
        ],
        "has to be greater than or equal to the threshold in the same will be true of WW1. fourth row third row and then the last row says W 1 * 1 and + W 2 * 1 has to be less than the threshold. So the threshold is positive the two weights or bigger than or equal to that. So they're positive and they have to be bigger than ": [
            180.5,
            213.2,
            3
        ],
        "have done. And in what follows I'm going to assume Alphas one, so I don't have to deal with it. Yeah. Yeah, so active just means that this is one in in the Boolean case. Inactive means at 0 so if it's active it's going to make something change if it's not active it won't so this is just the rule for 1 weight and we're going to do this ": [
            1363.7,
            1399.1,
            36
        ],
        "head and I had that picture me to the weights and if I showed a picture of you and it fires a banging on the head and subtract that image from the weights. And so what you should end up with is a ghostly if you bought the weights as an image, which you can do because there's as many weights as pixels. You should see it kind of ghostly ": [
            1214.2,
            1236.1,
            31
        ],
        "here just the vector without the bias and there's a couple of points here to make. So it's a line in 2D. So why is x equals zero then you get that and I usually drive slope-intercept form here, but you guys are supposed to do it till I'm not Tough nuggets. Okay, so I took that out cuz it's your homework. So for 2D it's a line. Why is ": [
            1826.0,
            1861.4,
            48
        ],
        "if K is the one that makes this biggest. Okay, I like to use the yard Max notation just cuz it's a little simpler than going Mumble Mumble Mumble wife K is bigger than everything else arginmax. If you haven't seen it before it just means the J that makes this maximum. overall the from 1 to 10 and so it Returns the J that makes maximum and we'll call ": [
            2036.6,
            2070.6,
            54
        ],
        "in the by us. How much should we lower them by that's called The Learning rate and let's just make it one. So that becomes -1 actually these don't change because the inputs they're not active inputs there 00. But the bias always has an active input. So the bias is often the first thing learned by a neural net. Okay. Now I'm going to randomly pick another example of ": [
            760.2,
            793.0,
            19
        ],
        "is a linear discriminant makes you makes it clear what you should do with multiple categories. Pick the category with the strongest evidence. And then here's a quicker question. The guy on the left is Jeff Hinton Frank rosenblatt attorney on the Frank rosenblatt in the guy on the right is the most by Aaron. Rosenblatt, NC. If it's PNC, cuz those are the same thing. Okay. Alright, so we've ": [
            2310.4,
            2347.4,
            61
        ],
        "is input space. So here's two inputs, you know one ones up here 0 ones over here one zeroes down here zero Zero's there. So for 2D it Salon it. So what it does is everything over here. It's off everything up here. It's on so it can only solve linearly separable problems. That's that's what it can do. Okay, so and now I have to go back and get ": [
            1668.2,
            1710.2,
            44
        ],
        "is is great. But how do I program that? Instead. I'm going to make the learning roll that. so This is the Delta rule. It's called the Delta rule because you're taking the old way and you're adding to it. Some learning rate times the difference between what what you did and what you should have done. That's the teacher minus the output times the input on that line. And ": [
            1304.1,
            1337.0,
            34
        ],
        "it k Okay. And we couldn't have done that if we'd stuck with the output of zero or one. Okay. So this is what it looks like. You just have your inputs with your bias. And your outputs and each output has its own set of weights. So each one is like a perceptron now, except you're taking the weighted sum of the inputs and using that to decide which ": [
            2070.6,
            2105.1,
            55
        ],
        "it only matters if it's online learning, so you're learning on a pattern at a time and then you wanted to be randomized. No, I'm pretty sure it did not. Probably win the key age get to it. I don't have it. ": [
            2851.1,
            2882.3,
            72
        ],
        "it's in just by using the one or zero and but that makes generalizing this idea the multiple categories simpler. So here's the idea suppose we have 10 digits just to pick a random example, then we'd have 10 discriminant function. So wife gave X you'll have a set of weights. For that particular category for the 10 categories. And now we make decisions ex's assigned to class c k ": [
            1998.9,
            2036.6,
            53
        ],
        "just set the learning rate to 1 generate some samples. You know, that's that's too big. Let's try to make it even worse goes off into never-never land. So this used to be a coming back. Sac not going to come back. And now I can you know makes up perceptron stop wiggling around by just making learning rate smaller and smaller. Okay, so that's that's the kind of intuition ": [
            2621.4,
            2667.0,
            67
        ],
        "just tack one onto the front of your design Matrix to come of ones and then you have something it just has a very simple interpretation. Sometimes I'm going to include w0 on the weights and sometimes I'm not that's just to confuse you. Okay. So here's the learning rule if the output is one but it should be zero. What I should do is lower weight stack of inputs. ": [
            563.5,
            595.0,
            14
        ],
        "k. Okay. so a perceptrons a single layer Network the Opera 01 and it's a class fire and it's a linearly it classifies linearly separable thinks the way factor determines the orientation of the boundary and the bias determines where along that weighed Vector the separating plane is Hey. You supposed to think about that, okay. So anything's a perceptron can compute its can learn to compute thing. If it ": [
            2264.4,
            2310.4,
            60
        ],
        "kind of unit that only fires 400 that I have another unit that only fire. 401 and I want that to be on and and that's how it goes. So you have a hidden unit for every line in the truth table that only turns on for that particular input pattern and then you have my sweater plus one depending on what What the target is? So that requires an ": [
            285.0,
            319.6,
            6
        ],
        "line. So here's our knrj, this is this is where our knrj are equal. And here's where there's another one and it turns out that this picture is in fact the way it works, so What can we say about the point here? class well, but what can we say about the wise? They're all equal. So why I Y kyj are all of the same is that point? Okay, ": [
            2137.4,
            2188.9,
            57
        ],
        "look like this. There's no like funny holes in it or anything and you can prove that. I'm not going to make you do it, but if you think about Any point exits between these two and has a form like that? It's some fraction that one plus some other fraction of that one and You can go from there to figure out sit. That one is still in our ": [
            2234.7,
            2264.4,
            59
        ],
        "looking picture of me, which is really the difference between me and everyone else. Okay. And you guys are going to see that mirror images. Okay, so it's supervised learning. It's Error correction learning. We only punish it. We never praised it. So it's not very psychologically plausible and the patterns were presented randomly. Haha. I presented him that way so it would converge in class time. And it's really ": [
            1236.1,
            1274.9,
            32
        ],
        "of w. Okay, and that's what a part of your homework. Okay, so we can think of a perceptron as a linear discriminant. a two-class discriminant function is one where we decide the taxes in category 1 if Why is X is greater than or equal to 0 L sits in Category 2? So now instead of the output being 1 or 0 we're making a decision about what category ": [
            1965.3,
            1998.9,
            52
        ],
        "off I subtract the input Vector from the weights and that's going to make them point in opposite directions. So it'll tend to be off when it sees something. I'm supposed to be off for so if I train this thing to be a Gary detector, I'll have to wait for every pixel. I'll show you a picture of me and if it doesn't for a bang it on the ": [
            1191.3,
            1214.2,
            30
        ],
        "on for that. In fact, it'll be the right answer for everyone and when it's right, we don't do anything. So the weights of wandered around and wait space and found a place where it fits the data. Okay, and so we're done that's it's converged. That's why it's called the perceptron convergence theorem. so this was supervised learning. We gave it a set of input output examples told it ": [
            1053.8,
            1088.8,
            26
        ],
        "one of these is maximum. You're not outputting a 0 or 1. You're just out putting numbers and you're just picking the largest one. Okay. So this is now our decision X is assigned to class c k if it's the biggest one. Now if you're trying to If you're looking at a boundary between CI and CJ, that's where those two or equal. And that's going to be a ": [
            2105.1,
            2137.4,
            56
        ],
        "one's one plus one is two two is greater than equal to 2 so it fires. Okay, how about ex or? No, no, you keep guys are cheating. You've seen perceptrons before. Okay, so you can't find a set of weights at the store. And that's one of the things men ski and peppered pointed out in their book perceptrons. and so it's really easy to prove so you can ": [
            108.5,
            145.9,
            1
        ],
        "pick that one. I will give it a 01. And this way it is now - 1 that's my new network after that one. Tray example. Okay pics 01 and what's my output? How many people vote for zero? Not even the guy who said it, okay. But the target is one because 0-1000 is 1 * 008 + -1 - 1 - 1 is not greater than or equal ": [
            793.0,
            837.8,
            20
        ],
        "real time, and you're going to help me. We're going to start out with the weights being 000 Sol. Let me see. So I guess I'll do it down here. Here is why here is WW1 equals W 2 equals as a unit that's always one. This is one. This is ex-2 and this is W 0 Okay, so we're starting out with all these being zero. Okay, so I'm ": [
            678.9,
            723.4,
            17
        ],
        "really. You don't need to think of it that way. So let's get ourselves at these are the same role. So if the output lower the weight I put his one and should be 0-1. We're going to lower the weights. And so 01 that becomes one and we're just adding. right 3 - wise one if the Opera is wondering should be zero. I want to lower the weights. ": [
            1429.5,
            1470.2,
            38
        ],
        "slow because learning on some patterns screws up learning another patterns and turns out this can explain u-shaped learning in the past tense of English, but we're not going to talk about that cuz it's not cognitive science. Okay, you can talk to me later. If you want to know what that mean. Okay. So again now let's make it simple for computer science this you know English verbal stuff ": [
            1274.9,
            1304.1,
            33
        ],
        "so if this is one in this is zero, then you're adding that input to the weights some fraction of it. If this is your own this is one so you were on and you should be off your subtracting that input from the wake. That again. This is called the Delta rule because learning is based on the delta or difference between what you did and what you should ": [
            1337.0,
            1363.7,
            35
        ],
        "that actually turns out to be like a support Vector machine. Okay. If you know what that is, if you don't know what that is, don't worry. So it's supervised learning. There's a set of input patterns called the design Matrix, which just makes you sound smart at conferences set of desired outputs called the Target store. The teaching signal the network is presented with the inputs. What that's again ": [
            455.0,
            488.9,
            11
        ],
        "the same. We had 0 + 0 coming in they're not active in. The only one is this. Okay. I'm going to randomly pick another example. I'll pick just one and give it one zero. his input, this is our new network - 1 Okay - 101. Yep. Okay now 10 so 1 1000. What's the output? 0 okay and teacher says I should be one. So we're word on ": [
            925.4,
            971.2,
            23
        ],
        "the sound that the activation makes when it hits the output that's a technical term and the output is compared to the Target and if they don't match it changes the weights and threshold, so I'll get closer to producing the target next time. So first go to training set your design Matrix and the targets. But first let's make a little transformation. So everything will be a little simpler ": [
            488.9,
            523.7,
            12
        ],
        "the truck. Okay, so for example here's a you know 01. Here's one one is 10000. if we want to do or we descent the boundary somewhere like that, and we can easily separate the good guys from the bad guys if we want to do and we just set the boundary there and now it does and But now it's kind of easy to see intuitively why you can't ": [
            1710.2,
            1755.0,
            45
        ],
        "the weight and the separating line perpendicular to each other? That's what this diagram is showing take two points on this line x a and x b say, so they're on the line Y of x equals 0 so why the excavator equal 0 & y mx + b = hero and obviously then why have excessive a minus 5x vs Stihl 0 So wtx a minus WTSP equal zero. ": [
            1861.4,
            1894.4,
            49
        ],
        "then we can say something about how confident we are. You know, this one is just the same as this one, but now we can have a measure of how confident we are. What categories and you can even put out .5 it's totally confused. And in fact, we do this, right this will be the probability. The conditional probability of category 1 given the input. All right, so that's ": [
            2755.6,
            2787.8,
            70
        ],
        "this. I don't know why this is like this. Some hell I bet it might have something to do with what I was doing before. Okay. Let's quit out of last man that I've been start over. I think the simulation I was running before Seminole SEPTA the boundaries. That's better. Okay. So here's two categories. Now, these are clearly not linearly separable because there's some red dots among the ": [
            2479.3,
            2550.2,
            64
        ],
        "to 0 so we're off. Okay, so now I'm going to so this is okay. So what are we do I was off and I should be on I want to raise weights to active inputs. That means I'm going to change the the bias back to zero. WW2 was active so raised that way and now this is her new network. Okay. Now I'm going to randomly pick another ": [
            837.8,
            878.4,
            21
        ],
        "to compute. Trained on a function that it can compute it. It will always converge. So what can of perceptron compute? Let's do one more a little change here. I'm just writing Y is a function of X is just the inner product of the weights bigger than 0 Okay ball W and bold extra vectors face. So that should seem okay. Now wear this is equal to zero is ": [
            1592.0,
            1634.4,
            42
        ],
        "to make it a more modern version. There's the activation rule. If we subtract 800 from both sides now. You can think of the threshold as being 0. and then if you changed They might have stayed and dw0. That is a bias weight. Now you can have the bias weight be awake for me, you know if it's always one. And then you get a simpler equation. So you ": [
            523.7,
            563.5,
            13
        ],
        "two layers. Some people would call it one layer. Let's call a two layers today. This is the wonderful guarantee anything it can compute it can learn to compute. That's a very strong guarantee. The problem was there were a lot of things that couldn't compute but rosenblatt did assume nonlinear pre-processing. So he assumed some functions at a time that did some nonlinear manipulations of the data. And so ": [
            421.3,
            455.0,
            10
        ],
        "we need to raise things. So I'm going to give it going to raise this. This was an active input. This was not so we leave it alone. Okay, so this is our new network. It's starting to look. Turn off Chandelier. Okay. Now I'm going to randomly pick another example of pick this one and now if I give it to 00 The output is what? One teacher says ": [
            971.2,
            1014.9,
            24
        ],
        "weights. That's all it is. You're off and you should be on you add those weights those inputs your weights. Why because if you're supposed to be on adding that pattern to your weights that input Vector is going to make your input and your weight Vector line up and you're going to get a positive inner product and it's going to fire. If I'm on and I should be ": [
            1163.3,
            1191.3,
            29
        ],
        "what to do in every situation and the reason we do this as we hope things will generalize generalizing for a Boolean function doesn't really make sense. But for you know, if we train this thing to be a Gary detector, we would want it to generalize the new pictures of me. so I'm going to get a lot of exercise. Okay, so how would I turn it to be ": [
            1088.8,
            1123.1,
            27
        ],
        "what we're going to do next time. Let's go a little early. Yeah, yeah. We're on a we have to run to Changi point. And then the next time we can do it if you change the gravity between two again again. Yeah. Play that with regulations. The boy's mother is not effective learning because you go through all the training points in any way before you change the weights ": [
            2787.8,
            2851.1,
            71
        ],
        "where the decision changes from. category 1 to Category 2 or vice versa so you can set why is x equal to 0 and that is the decision boundary. Where are the output changes from 0 to 1 and that now has a simple geometric interpretation. Why is x equal 0 is a D minus one dimensional hyperplane in a d dimensional space? So here's a two dimensional space. This ": [
            1634.4,
            1668.2,
            43
        ],
        "you should have for what's going on. And what happens when you make the learning rate too big or too small? Okay, you make it to too small It's going to take a very long time even if you have a GPU. Okay, so, okay, so it's 6:12. I guess I don't really. Oh, I know what I can do. No, I better not do that. Okay. the next time ": [
            2667.0,
            2711.3,
            68
        ],
        "you're really separable. It's not even very clear. But this is let's let's make this. 6 and 6 and usually doesn't run those off the page like that. I don't know why it's doing that. But that means the learning rate is too high. It's changing the weights too much for each example. So if we slow it down. I can't even see this is so weird doesn't usually do ": [
            2438.0,
            2479.3,
            63
        ]
    },
    "File Name": "Deep Learning - C00 - Cottrell, Garrison W - Fall 2018-lecture_2.flac",
    "Full Transcript": "Listen to a podcast. Okay, if you're just joining us.  I now have a microphone.  Okay, and now is 0110 * 101 * 1 is one one is greater than or equal to one. So it's on and the rest are like that. Okay. How about a set of Weights in the threshold to do hand?  Yeah.  1 1 2  Yeah, so now this won't fire until we get both one's one plus one is two two is greater than equal to 2 so it fires.  Okay, how about ex or?  No, no, you keep guys are cheating. You've seen perceptrons before. Okay, so you can't find a set of weights at the store. And that's one of the things men ski and peppered pointed out in their book perceptrons.  and  so it's really easy to prove so you can even do it in class time. So if I needed to do the first row, that means the W 1 * 0 + W 2 * 0  would have to be less than the threshold.  So that means the threshold is positive.  How to get the second row I need W 1 * 0 + W 2 * 1 is greater than or equal to the threshold. That means W2 has to be greater than or equal to the threshold in the same will be true of WW1.  fourth row third row and then the last row says W 1 * 1 and + W 2 * 1  has to be less than the threshold.  So the threshold is positive the two weights or bigger than or equal to that. So they're positive and they have to be bigger than equal to that and then there's some has to be less than that not possible.  Okay, so you can't compute excellent Mexican passport said now if you had hidden units you could compute any Boolean function, but there is no learning rule for networks with hidden units and we don't think one will ever be discovered.  and by that they meant anyone that has the same kind of guarantee that a perceptron has and I haven't told you what that guarantee is yet, but basically leave it mostly as an exercise for the reader, but  So here's this and here's the X1 and X2.  If I can find weights and a threshold then I'll make that fire 400 which I can then I can it's supposed to be off. I can never mind. This one here 400 South got a kind of unit that only fires 400 that I have another unit that only fire.  401 and I want that to be on  and and that's how it goes. So you have a hidden unit for every line in the truth table that only turns on for that particular input pattern and then you have my sweater plus one depending on what  What the target is?  So that requires an exponential number of hidden units in so that's not very that's it. It's a good construction, but it's not very practical turns out you can do parody with you no end bit parity with end hidden units are linear number.  So that's what perceptrons are and again the goal was to make this new really inspired machine that could categorize inputs, but learn to do it from examples. And so  There's a story we didn't I don't know if it's apocryphal or not. But basically rosenblatt, of course the Army took pictures of Tanks. Some of the pictures had tanks in them. Some of them didn't you wanted to perceptron to tell you where there was a tank in the picture and it did that great. The problem was it turned out all the pictures with tanks were taken in bright sunlight in the ones with outward and it was just figuring out how bright the image was.  Didn't didn't generalize very well to dim pictures with tanks in them or bright pictures without tanks.  So rosenblatt discovered this learning rule called the perceptron convergence procedure and it's called that because it's stop straining after a while. It's guaranteed to learn anything computable. But to some people would call this two layers. Some people would call it one layer. Let's call a two layers today. This is the wonderful guarantee anything it can compute it can learn to compute. That's a very strong guarantee. The problem was there were a lot of things that couldn't compute but rosenblatt did assume nonlinear pre-processing.  So he assumed some functions at a time that did some nonlinear manipulations of the data. And so that actually turns out to be like a support Vector machine.  Okay.  If you know what that is, if you don't know what that is, don't worry.  So it's supervised learning. There's a set of input patterns called the design Matrix, which just makes you sound smart at conferences set of desired outputs called the Target store. The teaching signal the network is presented with the inputs. What that's again the sound that the activation makes when it hits the output that's a technical term and the output is compared to the Target and if they don't match it changes the weights and threshold, so I'll get closer to producing the target next time.  So first go to training set your design Matrix and the targets.  But first let's make a little transformation. So everything will be a little simpler to make it a more modern version. There's the activation rule.  If we subtract 800 from both sides now. You can think of the threshold as being 0.  and then if you changed  They might have stayed and dw0. That is a bias weight. Now you can have the bias weight be awake for me, you know if it's always one.  And then you get a simpler equation. So you just tack one onto the front of your design Matrix to come of ones and then you have something it just has a very simple interpretation.  Sometimes I'm going to include w0 on the weights and sometimes I'm not that's just to confuse you.  Okay. So here's the learning rule if the output is one but it should be zero. What I should do is lower weight stack of inputs. So that the next time I won't be on in that situation and here I stay active inputs. I mean that the input is R1 if the output of zero and it should be big. Should be one then raise the waist active inputs including the bias.  so  this is a very simple learning roll. So we're going to choose or and we're going to stop here for a quick demo on the board because he have to do this in your homework.  So  Deus Ex 1 years, that's 2 years that or of those two.  0 0 0 0 1 1 1 0 1 1 1 1  Okay, and now I'm going to make a little table x 1 x 2.  Out why the output tees at Target?  W0 W1 W2  And I'm going to train this thing in real time, and you're going to help me.  We're going to start out with the weights being 000 Sol.  Let me see. So I guess I'll do it down here.  Here is why here is WW1 equals W 2 equals as a unit that's always one. This is one. This is ex-2 and this is W 0  Okay, so we're starting out with all these being zero. Okay, so I'm going to randomly pick an example of pick this one and give it to Euro 02. What's the output?  Okay, this is to raise your acetylcholine. It's one zero is greater than equal to zero.  Okay, so the output is 1 the target is zero. And so the system says it's still your turn. Okay, so we were  And we should be off so we want to lower the weights in the by us. How much should we lower them by that's called The Learning rate and let's just make it one. So that becomes -1 actually these don't change because the inputs they're not active inputs there 00.  But the bias always has an active input.  So the bias is often the first thing learned by a neural net.  Okay.  Now I'm going to randomly pick another example of pick that one.  I will give it a 01.  And this way it is now - 1 that's my new network after that one. Tray example. Okay pics 01 and what's my output?  How many people vote for zero?  Not even the guy who said it, okay.  But the target is one because 0-1000 is 1 * 008 + -1 - 1 - 1 is not greater than or equal to 0 so we're off.  Okay, so now I'm going to so this is okay. So what are we do I was off and I should be on I want to raise weights to active inputs. That means I'm going to change the the bias back to zero.  WW2 was active so raised that way and now this is her new network.  Okay. Now I'm going to randomly pick another example of pick this one again.  And give it to zero Zero's input.  What's the output?  1  It's supposed to be zero. So 0 * 000 * 1 is zero a 1000 is greater than equal to zero. That's one teacher says it's still your turn. Okay, so we were on and we should be the next time it's e00. I'll be okay these word active inputs. So they stay the same. We had 0 + 0 coming in they're not active in. The only one is this. Okay. I'm going to randomly pick another example. I'll pick just one and give it one zero.  his input, this is our new network - 1  Okay - 101. Yep. Okay now 10 so 1 1000. What's the output?  0 okay and teacher says I should be one.  So we're word on we need to raise things. So I'm going to give it going to raise this. This was an active input.  This was not so we leave it alone.  Okay, so this is our new network. It's starting to look.  Turn off Chandelier.  Okay. Now I'm going to randomly pick another example of pick this one and now if I give it to 00  The output is what?  One teacher says I've told you three times now if you should be off in that situation, so what do I write here?  How about here?  How about here?  Okay, the bias is the opposite of threshold. So Stevens one. That was one one one. That was the threshold down 2-1 1-1. And now if I give it one one.  Wow, a generalized has something in her son before and it will be on for that. In fact, it'll be the right answer for everyone and when it's right, we don't do anything. So the weights of wandered around and wait space and found a place where it fits the data. Okay, and so we're done that's it's converged. That's why it's called the perceptron convergence theorem.  so  this was supervised learning.  We gave it a set of input output examples told it what to do in every situation and the reason we do this as we hope things will generalize generalizing for a Boolean function doesn't really make sense. But for you know, if we train this thing to be a Gary detector, we would want it to generalize the new pictures of me.  so  I'm going to get a lot of exercise. Okay, so how would I turn it to be a Gary detector? So I put in an image, which is just a table of numbers.  And you know, there's me.  And I have a wait for every pixel. That's this is what you guys are actually doing here and I  Every time now one thing it's too late. Well.  You can kind of see what I did hear is I just added and subtracted the input vector.  To the weights. That's all it is.  You're off and you should be on you add those weights those inputs your weights. Why because if you're supposed to be on adding that pattern to your weights that input Vector is going to make your input and your weight Vector line up and you're going to get a positive inner product and it's going to fire.  If I'm on and I should be off I subtract the input Vector from the weights and that's going to make them point in opposite directions. So it'll tend to be off when it sees something. I'm supposed to be off for so if I train this thing to be a Gary detector, I'll have to wait for every pixel. I'll show you a picture of me and if it doesn't for a bang it on the head and I had that picture me to the weights and if I showed a picture of you and it fires a banging on the head and subtract that image from the weights. And so what you should end up with is a ghostly if you bought the weights as an image, which you can do because there's as many weights as pixels. You should see it kind of ghostly looking picture of me, which is really the difference between me and everyone else.  Okay.  And you guys are going to see that mirror images. Okay, so it's supervised learning. It's Error correction learning. We only punish it. We never praised it. So it's not very psychologically plausible and the patterns were presented randomly. Haha. I presented him that way so it would converge in class time.  And it's really slow because learning on some patterns screws up learning another patterns and turns out this can explain u-shaped learning in the past tense of English, but we're not going to talk about that cuz it's not cognitive science.  Okay, you can talk to me later. If you want to know what that mean. Okay. So again now let's make it simple for computer science this you know English verbal stuff is is great. But how do I program that?  Instead. I'm going to make the learning roll that.  so  This is the Delta rule. It's called the Delta rule because you're taking the old way and you're adding to it. Some learning rate times the difference between what what you did and what you should have done. That's the teacher minus the output times the input on that line. And so if this is one in this is zero, then you're adding that input to the weights some fraction of it. If this is your own this is one so you were on and you should be off your subtracting that input from the wake.  That again. This is called the Delta rule because learning is based on the delta or difference between what you did and what you should have done.  And in what follows I'm going to assume Alphas one, so I don't have to deal with it.  Yeah.  Yeah, so active just means that this is one in in the Boolean case.  Inactive means at 0 so if it's active it's going to make something change if it's not active it won't so this is just the rule for 1 weight and we're going to do this by the same road all the ways, but I should say I can say here I might as well that it doesn't matter. This doesn't need to be 0 or 1 it can be any real number.  And then how big it is makes a difference and how much it's going to change the weights.  so, you know thinking about this is just doing Boolean functions is kind of  Not really. You don't need to think of it that way.  So let's get ourselves at these are the same role. So if the output lower the weight I put his one and should be 0-1. We're going to lower the weights.  And so  01 that becomes one and we're just adding.  right  3 - wise one  if  the Opera is wondering should be zero.  I want to lower the weights. So I'll put his lunch should be zero I get a minus one.  So how there should be an X1 here? Oh, yeah, this is assuming this input is 1  What affects is inactive I-80 then the weight doesn't change.  Okay.  and  Oh wait, that's that's an old slide. I should take that side out. What side is that? 27 remove slide 27. What about the bias Retreat? W0 is a weight from a unit that's always a constant one. So the Opera is one should be zero lower the weights and lower the bias.  This leads to subtracting one from the bias or subtracting learning right from the bias.  What if we get it right? Well if we get it right then TN, why are the same so this is zero and nothing happens.  Okay.  So one line of code to rule them all.  one line Divine them in the Darkness  Okay.  Okay, and X does not need to be binary in that case it changes the weights in proportion to its size.  OKC demo, and the demo is not going to show you the demo just yet.  Okay, still need that at these sides a little bit. I made a bunch of changes today. Okay computer and learn to compute.  Trained on a function that it can compute it. It will always converge.  So what can of perceptron compute?  Let's do one more a little change here. I'm just writing Y is a function of X is just the inner product of the weights bigger than 0  Okay ball W and bold extra vectors face. So that should seem okay.  Now wear this is equal to zero is where the decision changes from.  category 1 to Category 2 or vice versa  so you can set why is x equal to 0 and that is the decision boundary. Where are the output changes from 0 to 1 and that now has a simple geometric interpretation.  Why is x equal 0 is a D minus one dimensional hyperplane in a d dimensional space? So  here's a two dimensional space. This is input space. So here's two inputs, you know one ones up here 0 ones over here one zeroes down here zero Zero's there. So for 2D it Salon it. So what it does is everything over here. It's off everything up here. It's on so it can only solve linearly separable problems.  That's that's what it can do.  Okay, so  and now I have to go back and get the truck.  Okay, so  for example  here's a  you know 01.  Here's one one is 10000.  if we want to do or we descent the boundary somewhere like that, and we can easily separate the good guys from the bad guys if we want to do and  we just set the boundary there and now it does and  But now it's kind of easy to see intuitively why you can't do excellent cuz he's supposed to be on here on here off here and off here. And there is no way to put a line down here that separates this in this from this and this  everybody always kind of goes like that for a while, but  So that's why it can't that's another way of showing why it can't do X or so when you're doing happy vs. Sad and image space and image.  Remember we said an image is not 2D an image of the point in a high dimensional space where every Dimension corresponds to a pixel you're finding some separating hyperplane in that space that puts the happy guys on one side and the sad guys on the other.  Okay.  and  I'm going to abuse our notation don't report me, but I'm going to make W here just the vector without the bias and there's a couple of points here to make.  So it's a line in 2D. So why is x equals zero then you get that and I usually drive slope-intercept form here, but you guys are supposed to do it till I'm not  Tough nuggets. Okay, so I took that out cuz it's your homework. So for 2D it's a line. Why is the weight and the separating line perpendicular to each other? That's what this diagram is showing take two points on this line x a and x b say, so they're on the line Y of x equals 0 so why the excavator equal 0 & y mx + b = hero and obviously then why have excessive a minus 5x vs Stihl 0  So wtx a minus WTSP equal zero.  So w t x x a minus x p which is all segments of the line Y equals 0 so their inner product is zero. So there perpendicular.  So the weight the weight factor determines where the line is or if it's a plane where the plane is.  and then  The distance to the origin that is where you set your threshold kind of thing is El people in that again. We're WW1 through WD.  And you have to prove this in your homework.  So here's one kind of proof. Do you have to prove this to its the length minus? W0 over the length of w?  And in doing this you have to assume that the weight factor is pointing that way because otherwise he got a negative distance, but the answer is - W 0 / the length of w.  Okay, and that's what a part of your homework.  Okay, so we can think of a perceptron as a linear discriminant.  a two-class discriminant function is one where we decide the taxes in category 1 if  Why is X is greater than or equal to 0 L sits in Category 2?  So now instead of the output being 1 or 0 we're making a decision about what category it's in just by using the one or zero and but that makes generalizing this idea the multiple categories simpler.  So here's the idea suppose we have 10 digits just to pick a random example, then we'd have 10 discriminant function. So wife gave X you'll have a set of weights.  For that particular category for the 10 categories. And now we make decisions ex's assigned to class c k if K is the one that makes this biggest.  Okay, I like to use the yard Max notation just cuz it's a little simpler than going Mumble Mumble Mumble wife K is bigger than everything else arginmax. If you haven't seen it before it just means the J that makes this maximum.  overall the from 1 to 10 and so it Returns the J that makes maximum and we'll call it k  Okay.  And we couldn't have done that if we'd stuck with the output of zero or one.  Okay. So this is what it looks like. You just have your inputs with your bias.  And your outputs and each output has its own set of weights.  So each one is like a perceptron now, except you're taking the weighted sum of the inputs and using that to decide which one of these is maximum. You're not outputting a 0 or 1. You're just out putting numbers and you're just picking the largest one.  Okay. So this is now our decision X is assigned to class c k if it's the biggest one.  Now if you're trying to If you're looking at a boundary between CI and CJ, that's where those two or equal.  And that's going to be a line.  So here's our knrj, this is this is where our knrj are equal.  And here's where there's another one and it turns out that this picture is in fact the way it works, so  What can we say about the point here?  class  well, but what can we say about the wise?  They're all equal. So why I Y kyj are all of the same is that point?  Okay, and what about that point?  Yeah why I am. So this is think of this was input space.  And it's 2D in this case. And at this point why I is bigger than YJ. NYK and it turns out that every region is condex meaning.  That if any two points are in this region, then the point between them is in this region.  And so it look it really does look like this. There's no like funny holes in it or anything and you can prove that. I'm not going to make you do it, but if you think about  Any point exits between these two and has a form like that? It's some fraction that one plus some other fraction of that one and  You can go from there to figure out sit. That one is still in our k.  Okay.  so a perceptrons a single layer Network the Opera 01 and it's a class fire and it's a linearly it classifies linearly separable thinks the way factor determines the orientation of the boundary and the bias determines where along that weighed Vector the separating plane is  Hey.  You supposed to think about that, okay.  So anything's a perceptron can compute its can learn to compute thing. If it is a linear discriminant makes you makes it clear what you should do with multiple categories.  Pick the category with the strongest evidence.  And then here's a quicker question. The guy on the left is Jeff Hinton Frank rosenblatt attorney on the Frank rosenblatt in the guy on the right is the most by Aaron. Rosenblatt, NC.  If it's PNC, cuz those are the same thing.  Okay. Alright, so we've got 14 minutes left.  Let me just give up a little demo. I guess not this demo.  Let's see.  What am I looking for desktop?  Okay.  Quit work what happened? There should be some other stuff, too.  Okay.  so here's a little demo of a perceptron like  thing  that's interesting and let's try again.  Why is it not doing now?  720 area code  6  Okay, so  this is not when you're really separable. It's not even very clear. But this is let's let's make this.  6 and 6 and usually doesn't run those off the page like that. I don't know why it's doing that. But that means the learning rate is too high.  It's changing the weights too much for each example.  So if we slow it down.  I can't even see this is so weird doesn't usually do this. I don't know why this is like this.  Some hell I bet it might have something to do with what I was doing before.  Okay.  Let's quit out of last man that I've been start over.  I think the simulation I was running before Seminole SEPTA the boundaries.  That's better. Okay. So here's two categories. Now, these are clearly not linearly separable because there's some red dots among the blue dots where you can do in cases like that. There's something called the pocket algorithm where he keeps the best weight so far in your pocket, or you can just make the learning rate smaller and smaller until it stops and I'll still make mistakes but at least  So that's the that's the separating plane.  And it's going to keep doing this as long as I do this until because the guys near the boundaries are changing the weights too much by may make the learning right a little smaller. It slows down. I make it a little smaller.  It's smoother.  And I can even have the learning rate be that small and it's still.  Still gets there eventually.  But you know, but you don't want your learning rate to be too big to start. So what if we just set the learning rate to 1 generate some samples.  You know, that's that's too big. Let's try to make it even worse goes off into never-never land. So this used to be a coming back.  Sac not going to come back.  And now I can you know makes up perceptron stop wiggling around by just making learning rate smaller and smaller.  Okay, so that's that's the kind of intuition you should have for what's going on. And what happens when you make the learning rate too big or too small? Okay, you make it  to too small  It's going to take a very long time even if you have a GPU.  Okay, so, okay, so it's 6:12. I guess I don't really.  Oh, I know what I can do. No, I better not do that. Okay.  the next time  This is fall 17. So it's probably not going to change very much.  We're going to talk about logistic regression and multinomial regression.  And the basic idea is that instead of the output being this.  Bang bang kind of things. There's going to be some nonlinear function here. So instead of  Instead of a function that looks like this. We're going to have a function that looks like this.  And then we can say something about how confident we are.  You know, this one is just the same as this one, but now we can have a measure of how confident we are. What categories and you can even put out .5 it's totally confused. And in fact, we do this, right this will be the probability.  The conditional probability of category 1 given the input. All right, so that's what we're going to do next time. Let's go a little early.  Yeah, yeah.  We're on a we have to run to Changi point. And then the next time we can do it if you change the gravity between two again again.  Yeah.  Play that with regulations. The boy's mother is not effective learning because you go through all the training points in any way before you change the weights it only matters if it's online learning, so you're learning on a pattern at a time and then you wanted to be randomized.  No, I'm pretty sure it did not.  Probably win the key age get to it.  I don't have it. "
}