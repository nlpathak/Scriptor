{
    "Blurbs": {
        "Behavior. Hey. All right. so so we have this monotonic activation function and here again, I'm separating out the bias. So it's the inner product between the weights and the input plus this bias term and G is the sigmoid. Yeah, so okay, you can also Imagine the same network being linear regression Network where G would be the identity function. And then were were actually plotting a line in ": [
            1629.9,
            1680.7,
            36
        ],
        "I guess this W or this W. This has to be negative because it's a distance. So minus a minus is a plus if the weight Vector was pointing over here there wouldn't be the solution at BW 0 instead. Joe there'd be no minus sign in front cuz it's a distance. So this confused the students last year and then I realized well this example they're assuming it points ": [
            1173.2,
            1210.9,
            25
        ],
        "I'm not sure but it's it's a it's great nothing like it but the theme song is do do do do do do do do and that looks like Who are something weird is going here in the Twilight Zone? Okay, so I didn't tell you this but linear regression you're trying to draw a line through some data, you know if you've got some points. And you're trying to ": [
            2981.0,
            3017.2,
            73
        ],
        "It doesn't work as well as if you minimize a different objective function, which is cross entropy. And so when instead of some squared are you try and minimize cross entropy? You got this. Dudududu dinner. Okay, and so but that's your homework is to try to take that derivative. And what are you going to have to do? well when you get to the part where When you get ": [
            4380.2,
            4417.2,
            111
        ],
        "Listen to a podcast. Okay, let's get started. check Okay, so this is the first lecture and from what I understand nobody understood. Is that is that right? Okay, so I just wanted to like point of few things out. Okay, so At first we took one feature of A's and B's and we figured out. Okay, so this is the feature and we're going to put a threshold somewhere ": [
            1.9,
            180.4,
            0
        ],
        "Okay. All right, and I also showed although I didn't really show you that that guy and that guy. Is that Jeff Henry on the Coon? Okay. I didn't really show that but you can tell you can do face recognition really well and you can tell those aren't the same. Okay, so that's it for today. No problem. Are non-linear equations? You can try. Are we going to talk ": [
            4485.6,
            4560.0,
            114
        ],
        "Okay. Okay, I have to be able to remember what I wrote over there when I get over here, but instead. What can I do? Okay. Now I don't have to remember what I said. Ok - t - y. time's the derivative of e tea with respect to y s r e t respected WI Minus the derivative of y with respect to Wi, okay, the derivative of the ": [
            3638.3,
            3696.3,
            91
        ],
        "So there's a generalization to perceptron called logistic regression at places where places that buy that which allows us to go from zero one class for class first gives us a probability of category membership. And unlike linear regression for logistic regression. There's no closed form formula anymore. It's not linear equations anymore. And so we have to use gradient descent. Okay, and I done traded gradient descent for linear ": [
            4307.2,
            4348.0,
            109
        ],
        "The linear combinations you can put sweet. Yeah, so this is a linear combination of the inputs, but it's like the inputs to different powers. And since and we need something we're trying to minimize. Until we get we start with this as if they were trying to minimize as some squared air and it doesn't matter what consonants are out here or even if it's 1 / n which ": [
            457.8,
            490.0,
            8
        ],
        "Vector shorter Occam's razor? Yeah. You don't need less weights. You need smaller weights. So it would stay would fit in a little box as opposed to Big Box. Okay, so that's what I'm talking about here. And we're going to you're going to do that in your homework your programming assignment. I think do we have that in the programming assignment? I don't remember. Okay play here lambdas, just ": [
            780.4,
            822.3,
            17
        ],
        "W 1 x Point whatever it is plus W210 whatever it is squared Etc. So it's it's all constants except the W's Yeah. So let me just stop for a second and say in some sense the XX Square... X to the m are the features this time, right? So we just take the features and takacs and we Square it we cubed cornet get whatever. Does that make sense? ": [
            404.0,
            455.2,
            7
        ],
        "We figure out what that is and then we going the opposite direction. So this is that little change of w that's making the error smaller. This is the formula for gradient descent doesn't matter what whether we're talking about W or some other model some huge complicated model with millions of these guys. We still are doing this. We're going downhill in the air. So how do we do ": [
            3382.7,
            3415.6,
            84
        ],
        "a and which has a big Matrix and and these are the W's Oh, actually it should be the other way around yet. So let's say w x equals the targets. Okay now to get what w is I have the targets X the inverse of x. Well, when all these are matrices and X is not Square its rectangular then you do the pseudo inverse of X and that's ": [
            3161.2,
            3198.4,
            78
        ],
        "a if the exes are a little if the Sun is a little bit bigger will say it's category 1 if it's a little bit smaller will say it's Category 2. And so this is where the line is. It's where Wyatt of x equals 0. So this is a linear separator because there's a line here. And the weight Factor now in this setup is two dimensional the data ": [
            1018.4,
            1049.2,
            21
        ],
        "a kind of motivation for the sigmoid. Yeah, cuz otherwise, she'll Hell Breaks Loose. It's not so simple is this. So in other words, we can interpret the output of the sigmoid as a probability the posterior probability. It's called in spaceland. It's probably a category 1 given X and then that thing can be written as something that looks like. The weighted sum of the input supposed to buy ": [
            2071.1,
            2121.8,
            48
        ],
        "a line to the data. so what are we do? We want to minimize this and we have all these parameters. and the update rule for gradient that says wi is the old wi so this is one of our parameters - the partial derivative of the SSE. Weather expected that parameter. That's exactly what I've been saying all on this is the slope with respect to that one parameter. ": [
            3345.1,
            3382.7,
            83
        ],
        "about how? Welcome to Facebook. Yeah, that's me. Some of the coolest stuff when when it's like holyshit now. I was reading up on there like while ago and I'm going to teach about it, right? You will be jumping the gun as they say that's all right. I saw that we are using L2 Norm or UC San Diego podcast ": [
            4560.0,
            4796.2,
            115
        ],
        "all over the place. right So I just plugged it back in. And a is p of x given see one P2P of c and 1 and over this and B is probably a C2. Blah blah blah blah blah, okay. So I just was simplified it by calling this thing a / A + B and then when I got here, I plugged those things back in for a ": [
            2213.2,
            2251.7,
            51
        ],
        "all the other parameters are constant that gives you partial derivatives turns out that's the only difference and that tells us which way is up in the air. But we want to go downhill. So we take the negative of the slope and add that to the parameter. Here's the idea. You should have in your head. at least four Like I said, here's some parameter wi and we imagine ": [
            2421.8,
            2455.3,
            57
        ],
        "all the other parameters are fixed it whatever their value is right now so I can plug wi into the formula for sum squared error and for some values of wi it'll be high and for some values of wi nobilo. So this is SSE and in fact because it's some squared error. It'll be a parabola. In terms of this parameter. So this this is the sum squared error ": [
            2455.3,
            2489.7,
            58
        ],
        "and b. Yeah, okay, who's at the desk that okay. Okay. Okay, so that's really nice, but we can't assume that our data is going to be gaussian. If you know if we happen to know they're gassing they have this mean in the same variance, then I can just set the weights and I'm done. I don't have to train a neural network at all. But we don't know ": [
            2251.7,
            2284.1,
            52
        ],
        "and you know, unless your Jimi Hendrix. You don't think you can change constants. You know, he said if 6 turns out to be 9, I don't mind. Cuz you got your own world to live in and it ain't going to bother me. He is the Target tea for Target tea for teacher. bike xr100 Okay, so we don't want to change any constants because then everything will go ": [
            3854.2,
            3890.9,
            96
        ],
        "are and we'll get the minimum sum squared error and but we do this for all of the parameters at the same time. And that's what we do now usually this. Distance that we go is is the soap and it usually is love going to be really big up here. So you have a learning rate. That's how much of this we do. And so if the learning rate ": [
            2554.9,
            2584.6,
            61
        ],
        "are the bees. Okay, just imagine that's what happens right? So this could be you know the value of the feature in a few remember and go back to the first lecture where he had histograms their kind of gassy and ish just like I'm vegan ish because I eat pizza. So this could be your midterm grade. Right? And this is you guys and this is some monkeys taking ": [
            1719.0,
            1754.7,
            38
        ],
        "as I've been used to learn machine translation from English to French French to English Setter Etc has been used for playing training a system to play Go Fish things are going to drive your car soon later in the quarter will see how you can even have a system learn to program learning program to recognize faces and emotions object recognition image captioning generating new images of faces that ": [
            2317.7,
            2359.8,
            54
        ],
        "as a function of wi to function no place where it has two values. Okay. So suppose we start out here. What I want to do is compute the slope of this thing that tells me which way will increase the Earth. So I take the negative of the slope and that's just some number, you know, it's the rise over the Run. Right and that's going to be some ": [
            2489.7,
            2523.7,
            59
        ],
        "be different in this line would be over here. Just cutting off one one. Okay. And given the solution we found like one 1-1 it would be exactly at 45 degrees here. Right and -1 would make this one over that. What should be the square root of 2? Okay. Any questions now? What is a W-9? Why is W not have to be negative all the data's over here? ": [
            1123.2,
            1173.2,
            24
        ],
        "bigness but it can't be too small. It's got to be just right and you have to change it as certain rate the rate you change the learning rate. Is he start out with some learning right? Say alphazero and then you go through all the data you change the weight. Then you take Alpha 0/1 that doesn't change much. Then you go through the data again. The second time ": [
            4113.4,
            4147.0,
            103
        ],
        "changes more quickly in a neural net because X in the neural net is usually some number between 0 and 1 or 1 or -1 and so it's it's not always a very big number. Where is the bias will change quickly depending on the difference between the Target in the inputs. Yeah. Is that a question or Yeah, yeah. So again celebrity rate. Yeah, this is is essentially corresponds ": [
            4047.2,
            4078.5,
            101
        ],
        "derivative of the weighted sum by using the chain rule. So derivative of y if x was inspected wi is y Prime of x X the derivative of x with respect to W. I wear this is the weighted some of your inputs. But if if everything works out just right that'll cancel out with something else and you got the chain rule you get you get the Delta roll. ": [
            4451.9,
            4484.4,
            113
        ],
        "distribution. These are the probabilities of being an axe have a high probability of being here low-probability being here in here. That's the probability density function of the sea ones and this is the probability density function of the sea to say it's that one And that's just the formula for a gassy and the only difference between them is the mean new one here Mewtwo here. So me one ": [
            1821.0,
            1854.7,
            41
        ],
        "do. So, you know by scaling, you know, if you have one over and then the sum is a lot smaller. It's the average not the total sum squared error. And now if I change the learning rate I can account for that. So cuz it's going to scale the weight changes. Okay. Okay, so I haven't told you this but linear regression is when you're trying to sell it ": [
            2870.2,
            2909.8,
            70
        ],
        "equal to zero. So the two is going to come down. I'll get y - t and then some more stuff depending on what why is In this yeah. So when I plot this red line. So, okay. So what I do is I take the derivative here. I set it to zero and then I plug in All My Ex's so I'm going to get one equation for the ": [
            566.4,
            600.9,
            11
        ],
        "error, which if you take the square root you get euclidean distance. We're going to find the line that's closest to all the data. Okay. So this is a weird way to do linear regression if we got a closed form formula. Why would we ever want to do this? any ideas computational say what? Why does it? Yeah, so if this is the year of a big data or ": [
            3936.4,
            3976.6,
            98
        ],
        "everywhere and we Her Trying to minimize the difference between this and tea and we'll get an equation in m + 1 unknowns. Okay, the unknown to the W's now we take a second example, we got an equation and plus one in their rooms. Every Point here generates an equation. That's a polynomial. Where was a polynomial until we plugged in and now it's just like w 0 + ": [
            364.7,
            404.0,
            6
        ],
        "find a line that's as close to all the points as possible. And it turns out there's a closed form solution for this. He's been just invert a matrix or convert use a pseudo inverse and boom you're done and that's why you can call a Matlab function for linear regression and boom you're done. There's no gradient descent, but you can do gradient descent and it turns out with ": [
            3017.2,
            3047.1,
            74
        ],
        "first X. Annatto, that should be X soup in one equation for the second X1 equation for the third acts. I'll have an equations in whatever number of unknowns my W's are so if I have you know a linear thing I'll have w0 in WW1. So I'll have an equations in two unknowns. That's too many equations. But there's a linear algebra trick that still will minimize it so ": [
            600.9,
            633.2,
            12
        ],
        "fit the data perfectly. 1 2 3 4 Yeah, solve 9 equations and nine unknown so I can solve those using pseudoinverse and I'll be done. Follow all of that. This is just an example in order to get to. You know the idea of overfitting we're not going to do any polynomial fitting ever. In this class. Okay weird. It's just an example. Okay..... Somebody asked what's Lambda Lambda ": [
            668.0,
            713.0,
            14
        ],
        "for a bit of Y with respect to Wi. Okay, and now what's why why is the inner product of the weights for the inputs? So that becomes T-minus why is the derivative of the Sun as I goes from 0 to D of wixi with respect to Wi oops, I better use a different variable. How about Jay? Jay Jay okay. So what's the story of it is? X ": [
            3728.4,
            3771.0,
            93
        ],
        "function cuz it takes the line and squashes it down. It's also called the sigmoid which is Greek for ass basically or S. Like if you tell it your ad this looks like an s Okay. Sure, it's okay. That's the activation function and it's nice and smooth instead of the perceptron which isn't smooth. It's got a discontinuity and since we're going to use calculus and take derivatives if ": [
            1555.6,
            1592.1,
            34
        ],
        "go down the right direction. Doesn't matter actually so I can put one over N and is fixed for my training set. You know, maybe I've got a hundred example, so it's one over a hundred. That's the average if the minimum is going to be in exactly the same place. I don't remember what I told you to do in the assignment, but whatever I said, that's what you ": [
            2831.9,
            2870.2,
            69
        ],
        "going back and forth over here. If you're learning rate is higher than that. You'll get down to some point where you're jumping across here and it stays High. I know if he then lowers learning right and I'll go there if you keep lowering learning rate. I'll jump back and forth over here and then Okay, yeah. No, not really. But this is the nice thing about Square there ": [
            2648.1,
            2685.4,
            64
        ],
        "going to be the same thing. So we want smaller. So we have this expression that we did with the polynomials the sum squared are we want to make it smaller? So we'd like to change our parameters in such a way to make the arrow smaller. And the idea is to take the derivative of the sum squared error with respect to one of our parameters. And you assume ": [
            2390.5,
            2421.8,
            56
        ],
        "going to do logistic regression where y of x a where a equals the weight of town of the inputs from equals 0 2D of WI that's a and why has some I'm just writing this because we call this the activation function for perceptron. It's that weird graph for logistic regression. Its 1/1 + e to the minus a And that's shaped like this. It's called the squash and ": [
            1499.8,
            1555.6,
            33
        ],
        "good enough. For government work. Yeah, yeah. I am I doing this Saturday of layover every yeah, that's bad. If the features are correlated. We'll talk about that and a couple weeks. But yeah on every step I go through I can go through all the patterns. That's Bachelor ending figure out which way is the add all those little weight changes up. And so you go down. So the ": [
            2711.7,
            2762.3,
            66
        ],
        "got a formula for weather what the probability of it being in category 1 is so if I said call that a happy then I can divide through by a okay, then I can. take this and say oh, this is the e to the log and be over a which is just be over a Hi and then I can say oh no, its 1/1 + e to the ": [
            1956.9,
            1990.6,
            45
        ],
        "have to invert a matrix if you can avoid it. Okay. Hey other questions. Oh, it's it's the exactly the same right? I didn't I didn't say anything about the bias. We got to do something else know the bias is awake from units. It's always one. And so w 0 equals the old W 0 + t - y. * 1 so this is why the bias off and ": [
            4005.8,
            4047.2,
            100
        ],
        "have two dimensions. Okay. Okay. Okay part of your homework. Okay, we have this thing where y of x equals 0 that's but y of x is equal to w wtx for WW1 X1 + W2 X2. That's the inner product of the weight factor and hacks and I'm leaving out the bias here. And so that's equal to zero. Midwives XC that's where where this is zero. If it's ": [
            950.7,
            1018.4,
            20
        ],
        "here and it's a high probability of being a b. Okay, so that's and so What this says up here. I used to have all blue slides with people said that hurt their heads. So I didn't get all the way to switching this to these are just cut out from the other side. So the probability of an x given that you're in C1. So here's C1. It's this ": [
            1785.2,
            1821.0,
            40
        ],
        "here in anything less than that will call category one is in greater than that will call Category 2. Okay. This now, let's say we get a second feature. Okay. And now all of the category 2 or most of it is over here and category one is over here this and we put a line between them. That's a linear discriminant. Somebody said what's a linear discriminant? I meant ": [
            180.4,
            221.8,
            1
        ],
        "i y because we're assuming that all the other W's are constants when we're doing a partial derivative. And so and this is constant because it's given to us in the training set. And so the only part of this song that's not zero is the one where Jay equals I and so we have the derivative wixii over wi which is just x i Oh, you're you're talking. Thank ": [
            3771.0,
            3804.6,
            94
        ],
        "in the denominator WI. And that's why I put the 1/2 there is the derivative of T minus y. I'm sorry. I don't need this year Wy. Already, it's starting to look kind of like the the Delta Road got that Delta there. So all I did was use the chain Rule and take the river this thing and then I take the derivative of the thing inside of it. ": [
            3602.7,
            3637.4,
            90
        ],
        "in there and I had to say off we're on and we're supposed to be off and I want to raise the weights and lower the threshold for off and we're supposed to be on I want to raise the weights for were the threshold. I think I said the same thing twice, but were the bias. I just say, I raise the weights in the bias or a lower ": [
            1257.5,
            1276.8,
            27
        ],
        "into the sigmoid pretty much following along. But anyway now I've got a logistic regression. categorizer So as that thing. Which was this thing? applied at the logistic already another words to probably class one follows a sigmoid as a function of the log ratio of the probability of class seated press that is the law God's basically cuz these are going to cancel because they're usually equal So that's ": [
            2023.8,
            2071.1,
            47
        ],
        "is right here. Mewtwo is right there. Okay using Bayes rule to figure out okay, this is the probably the data given the category, but what we really want is probably the category given X. And so using Bayes rule, if you don't remember Bayes rule go look it up. I guess it's a probably x given C1 times the probability of C1 over the probability of the data. okay, ": [
            1854.7,
            1887.5,
            42
        ],
        "is there's a global to local minimum in the global minimum are the same once we get into neural networks with multiple layers are going to look so pretty if you think that looks pretty it'll be like this and you're going downhill and you can end up here when the best answer is over here, but the when happens is that most of the time a local minimum is ": [
            2685.4,
            2711.7,
            65
        ],
        "is too small like I showed you last time you don't get there very fast. If the learning rate is too big. I might actually jump over here and increase the air. And that's what happened when I set the learning rate to to on that perceptron thing in it. And the separating line just left the left the stage. Yeah. Yeah, so you pick some value like or no ": [
            2584.6,
            2619.2,
            62
        ],
        "is two-dimensional X1 and X2. And so if this is the equation of y f x equals is 0 We're going to call everything in the dataspace over here category 1 and that's Category 2 and the point of this slide is to show that the show this is a linear discriminant. The W says where the separating line is, it's always at right angles to it and the bias ": [
            1049.2,
            1088.6,
            22
        ],
        "it can compute only linearly separable things. And if it was 3D then we have a plane and if it's 4D we have a hyperplane & Beyond hyper playing all the way. All the way down or up. Okay. butcher to okay Witcher 2 since we're since I program and are used to program back when I programmed and see we count from zero cuz we're computer scientist. Okay. So ": [
            1405.3,
            1456.6,
            31
        ],
        "it, you know, let's let's assume that so y a x Is xn his WT? Xn Rachel hear X is a vector W is a vector and that's that's our model of the data turns out to be a liner a plane or hyperplane etcetera and we want to get it as close to the data as possible. We're not doing classification. We're doing regression here. We're trying to fit ": [
            3306.7,
            3345.1,
            82
        ],
        "jucunda data, whatever and so if I have a matrix this big and I have to invert it. It might not even fit in memory. But if I can take some examples and change the weights for every example using the Delta rule, then I could do it iterative lie, and I won't run out of memory. I'm not trying to invert a big Matrix again. You don't want to ": [
            3976.6,
            4005.8,
            99
        ],
        "just says how much we care. So one approaches to get more data and then I've got so much to say to the polynomial has to come as close to it as possible. But the other approach to making the model fit the data better is to have something else we minimized. What is this do if we want to minimize this were trying to shorten the weight Vector which ": [
            713.0,
            740.9,
            15
        ],
        "linear regression does is minimizes the sum squared error. That means Burger. You know, it's like a ax equals be right if I want to know what exit is I set x equal to be? x a 2-1 Okay now replace all these with matrices. It's a big Matrix. She's you know. Yeah, I mean like this is W, right and this is your parameters and you can have lots ": [
            3047.1,
            3100.3,
            75
        ],
        "looks kind of the same because you got this big smile, if you change the way it's as you go you may learn more by the end because you got a lot of redundancy in the training set. So that would be online learning where you got an example and you actually change the wait for that example. That's stochastic gradient descent. And so again, it's the Delta rule, okay. ": [
            4271.1,
            4306.1,
            108
        ],
        "makes it mean squared error. This formula will have the same minimum no matter whether we if we * 3 where its minimum in terms of the parameters will be the same place. Birthday doesn't matter if it's here or it's up here or it's down here. It's all the minimums always in the same place. So to minimize that thing which is going to be a parabola. Write this ": [
            490.0,
            523.2,
            9
        ],
        "me instead of like going Okay, so we want - so minus the partial derivative of the son of 1/2. I'm just hummus and goes from 1 to Big end of tea and mitos Y and quantity squared that's our objective function that we're trying to go down Helen Wright. And here by the way. This is our model. Now we're not having a yes or no or one or ": [
            3451.0,
            3490.0,
            86
        ],
        "mean squared error or whatever and I put a 1/2 in front to make things simple the mean Square. So I need an objective function just like for the polynomials. So I take n equals 1 to Big end of all the examples TN - y n squared okay, and that's what I want to minimize. if I want to do it by gradient descent Why has some W's in ": [
            3271.5,
            3306.7,
            81
        ],
        "means that this plus this has to some to one so we can put that plus the numerator. Sum that up on the bottom and that normalizes it. 2B, you know, we don't have to actually no P of x. Since we know these some of the one there's only two categories then that means that the denominator has to be this normalizing constant. Okay. Okay. okay, so now we've ": [
            1919.1,
            1956.9,
            44
        ],
        "negative log of A over B, which is just a minus and field. This is still a / B / B / a and then I can plug what I've got back in. This guy for one and that guy for the other in the key of X is canceled. And if I call this whole thing, I've got the signal. In fact, you can take anything and turn it ": [
            1990.6,
            2023.8,
            46
        ],
        "number and I subtract that number from wi and I get here now. I'm here figure out the slope figure out the negative of the soap by multiplying by -1 surprise surprise, and then I subtract that from W and at some point the subs going to get smaller and smaller and hopefully will converge to wear to minimum. That's gradient descent. So now we plug that into the substrate ": [
            2523.7,
            2554.9,
            60
        ],
        "of degree m. Where which things are the parameters? The wcso lot of people might think X. It's a variable so must be the parameters no exes the data. So, you know, we're going to get one point where X is here and tea. So this is x t is the target is here and what we do then is we that's point eight or something. We plug-in .84 X ": [
            324.6,
            364.7,
            5
        ],
        "of parameters and next is going to be your design Matrix with all the examples in this is going to be a vector not just be but a whole Vector of the targets for all those examples and then it turns out but it's not a square Matrix because you have more examples than parameters usually and then you have to do a pseudo inverse, which if you don't know ": [
            3100.3,
            3126.9,
            76
        ],
        "our date is gaussian the most things are gaussian, but you know almost everything is calcium in the world. We need to learn weights, but we're still going to interpret the output is the probability of category 1 given the input. So what do we do? What do we do? Tell me you know what we do. Gradient descent. Thank you. Sorry you got the slides from last. Okay, and ": [
            2284.1,
            2317.7,
            53
        ],
        "parabolas for one for each example. So if I just take one example and go downhill for that, it's not necessarily the same as the direction that I would get if I went through all the examples, so That's called stochastic gradient descent because you randomize the order the pattern so you get random examples and they go this way this way this way and on average they tend to ": [
            2795.1,
            2831.9,
            68
        ],
        "point one or .01. You don't want something bigger than one cuz then you're like Really going across that's why I said it to 2 and it went crazy. And typically though. What you're really doing is is Anil the learning rate, which is what I did by hand and that simulation it kept making it smaller and smaller than the line stop wiggling so much. That's because I'm just ": [
            2619.2,
            2648.1,
            63
        ],
        "provably minimizes the squared error. So I just have to invert a matrix. Now that actually can be iterative to do that. But yeah, that's all you have to do. make sense are we happy? Cuz cuz I'm never going to make you invert a matrix. If you have to invert a matrix, you should just go home and crawl into bed and cover your head with the covers. He's ": [
            3198.4,
            3229.8,
            79
        ],
        "really don't want to invert a matrix. Okay, so linear regression is saying I want to find this line. sell my model y equals c n o w x for all the EX's and all the the target. So I'm you know, I want tea and equals a w x n for lots of ends. right, and so And I do that by taking the sum squared error for the ": [
            3229.8,
            3271.5,
            80
        ],
        "regression, which can be useful for Big Data. Minimizing mean squared error leads to Delta rule for linear regression. But look I predicted. I was going to run out of time before I can show you how that generalizes the logistic regression, but that's your homework. Anyway, so in your homework, We don't use some squared error because when you do that with logistic regression you get a learning roll. ": [
            4348.0,
            4380.2,
            110
        ],
        "right here. It's too big essentially and this is the case where it's 1/2 the year and 1/2 - minimizing thing and it just spends too much time minimizing minimizing W and not minimizing the air and and so this is with no regularization. That's too low. Here's with too much regularization. That's too high. Here is the soup. That's just the right heat for Goldilocks. Okay. So that's what ": [
            822.3,
            867.9,
            18
        ],
        "scans or generative adversarial networks, which I finally did a lecture on last. For the crab students. I'll probably give it to you too. So what is gradient descent you have some objective function something that you're trying to minimize. How about minimizing errors seems like a good idea. That's what you want to do on your midterm. Right? You might as well minimize the square there right the same ": [
            2359.8,
            2390.5,
            55
        ],
        "so I've already done most of their your homework for you here. But when you get to the part where you got this? 10 - y + x the derivative of y with respect to Wi now I've got to why is no longer just to wait and some of the inputs it's the logistic function. And so we have to take the derivative of that and then take the ": [
            4417.2,
            4451.9,
            112
        ],
        "so that's the theoretical where you have to prove it. Most people use something like the square root of that. It makes it smaller and doesn't make the learning rates us too small too fast. Okay. Okay. So there's all this stuff in here you can look at. And that's what I just did mostly accepting this version. I forgot to put the 1/2 in front and I ended up ": [
            4173.6,
            4210.0,
            105
        ],
        "squared. It's a parabola in w. okay, so if I multiply this out the teaser constants the W switcher part of the formula are going to get multiplied times each other and I get w0w 0 squared etcetera. So it's a it's a function. I'm trying to get minimized and it's a parabola in the parameters. Okay, so to minimize that I take the derivative of this guy and Saturday ": [
            523.2,
            566.4,
            10
        ],
        "sum or the difference is the derivative of the is the sum of the derivatives. I got - t - y This is a constant. It's given to us by God and the training set. So, this is Zera. And now I've got minus the derivative of y with respect to Wi. Okay, and now I'm just going to get rid of those minus signs cuz there's two of them ": [
            3696.3,
            3728.4,
            92
        ],
        "sum squared error is the sum of the squared error over every pattern. So you have to do this forever and Eagles won to begin. And you add all those up? And then you go downhill or you can take one pattern one example and go downhill just based on that and because you're only looking at one example, it's not this, you know, this is the sum of many ": [
            2762.3,
            2795.1,
            67
        ],
        "tells us we're along that where that line is along that Vector. So if we were doing or and this was 01, and this was one zero and that's zero zero and this is one one. We want the network to the system return one. If all of those are on this side of the plane or the line and zero, otherwise if we are doing and the bias would ": [
            1088.6,
            1123.2,
            23
        ],
        "that are generated from the screen curve by adding and adding some normally distributed noise. You just have a gaussian. You don't go to conferences and call it a normal distribution with say a gaussian distribution with 0 mean so we just add little bits to the green line and we get that data. This is what we're going to try and fit to the data, which is the polynomial ": [
            295.8,
            324.6,
            4
        ],
        "that was any questions about that now. Okay. kfin just happened. Okay, let's see if I can remember what the confusions were here. That is Frank rosenblatt on the left. Okay, so why? why so a lot of people had a question about What's going on? Okay, what's what is this diagram, you know WTF Okay, so Does not do anything, okay? Okay. So what's going on here? We we ": [
            867.9,
            950.7,
            19
        ],
        "that's called an epic when you go through all the data and now I take that initial learning right in / 2 then so if you use Big T to stand for how many iterations you're doing you do that? How do you know if I'm on the teeth time? I'm changing the all the weights I / T that turns out to be too too aggressive and in fact, ": [
            4147.0,
            4173.6,
            104
        ],
        "that? We need to figure out this guy. Right, and that's the thing. We're going to add to W to make the error go down, right? So we want to know. What is -2 partial derivative? Of the air with respect to Wi, what is that? Okay, and the rest of the slides in this the side deck do this, but I'm doing it more slowly so you can follow ": [
            3415.6,
            3451.0,
            85
        ],
        "the class. So that's really easy to separate that's less. So so we're going to have the separated with a gaussian over with a logistic function. And what it's going to do is it's going to be low over here. And it's going to start to go up and reach about a half here because it's 50-50 whether you're in category or category B, and then it gets higher over ": [
            1754.7,
            1785.2,
            39
        ],
        "the derivative of a son that is the sum of the derivatives derivatives are linear writing linear operation so I can always do this and now in order to make my life easier. I'm just going to figure out one of these derivatives, you know, therefore men goes from 1 to n and I just found them up. So I want to just figure out what this is T minus ": [
            3534.0,
            3566.1,
            88
        ],
        "the line. It's a line segment and what I prove there was that w? W x that the inner product of those two is zero, and so they're at right angles. Okay. Okay. Alright, so. Is anybody still confused? Good. So what a perceptron is a linear separator it put linear because the line and it can only separate things where there is a line between them. So that's what ": [
            1356.1,
            1405.3,
            30
        ],
        "the same role that you want. If you're doing multinomial regression softmax regression, so Do do do do I'm so that's the theme song to The Twilight Zone by the way, you're too young for that. Don't watch it because parental advisory. But anyway, so they're pretty good. But do you want the Twilight Zone was Rod Serling introducing it? Anyway, you can probably find it on Netflix or somewhere. ": [
            2938.2,
            2981.0,
            72
        ],
        "the space to get as close to the data as possible. Okay, so we're going to give this counterintuitive. We're going to make this hard by giving this counterintuitive derivation. So that are two categories are gaussian. So What you should have in your head is that this is category 1. And like this is Category 2. So maybe this is some feature. And these are the A's and these ": [
            1680.7,
            1719.0,
            37
        ],
        "the weight Vector is the multipliers on the polynomial right terms. Lambda says how much we care about it if land has zero will have no regularization. If Lambda is 1 then we're going to try and balance these two to minimize them both. And if Lambda is 0000015 then we mostly care about this but we also want to make the weight Vector shorter. Why is making the weight ": [
            740.9,
            780.4,
            16
        ],
        "the weights in the bias. So it's few or characters on the slide. Okay. An end here I'm just showing that the separating line. That's what you had to do in your homework. You had to make this into slope intercept form MX plus b and I didn't drive it because he had to do it. And this is just a proof that this line is perpendicular to the weight ": [
            1276.8,
            1313.4,
            28
        ],
        "then probably see to is that so now if we know the prior probability of one category in the prior, probably the other and usually we just They're just one half but you can have data that's skewed in some way by there's a lot more of one category than the other and this the probability of the data. Then it turns out since these two some to one. That ": [
            1887.5,
            1919.1,
            43
        ],
        "then we call a category that category. Okay, any questions so far because you got if nobody understood it then we have to like stop. Someone said I could just go slower. So I think I might talk. Okay. Okay. So here's the second thing people didn't understand. This is a regression example where you're trying to get as close to some data as possible. Here is some glue dots ": [
            253.5,
            295.8,
            3
        ],
        "this way and that turns out to make W negative which means minus minus is a plus and we're okay. It's a distance. Okay. Alright any other questions? Yes. So the bias is always the option of threshold. So w 0 equals minus Theta. But we we made Theta go away earlier because it made makes the learning roll simpler to describe. So those are elected last year's slides data ": [
            1210.9,
            1257.5,
            26
        ],
        "through all the examples compute the way change, but you don't change the weights. Here's to another one change the way it's cuz I don't change the way she just some of them all up and then you take the average and change the way it's once this is typically a bad idea because a lot of times the data set is redundant in a while. So everybody's happy face ": [
            4243.5,
            4271.1,
            107
        ],
        "to hell. Okay. Okay, so that was linear regression. But in this case for linear regression T, you know, all your examples are going to be these points you're trying to fit and so particular X. This is T. It's not one or zero because it's linear regression for this axe. It's this tea Etc. So these RX tea pairs. Input output desired output and because we're using the squared ": [
            3890.9,
            3936.4,
            97
        ],
        "to learning rate of 1. Let's go back to So usually you speak in an alpha here. And this is the Robin's Monroe procedure from the 50s and they proved some things about what the learning rate oughta be. And how how are you should make it smaller and smaller and she go in order to ensure convergence. So they are there three. It's got to be a certain big ": [
            4078.5,
            4113.4,
            102
        ],
        "turns out I'm going to the next couple of lectures are going to be magical because and also really boring because I'm going to show that the perceptron rule the Delta roll for the perceptron is going to be the same rule that you want. If you're doing linear regression that's going to be the same rule that you want. If you're doing logistic regression and it's going to be ": [
            2909.8,
            2938.2,
            71
        ],
        "us if I set the weighted sum of the end. If I said wa equal to that end W 0 equal to this crazy thing and so I can interpret it as I can just read those. I can just look at the parameters of those two gaussians and set the weights. Right away. I just plug into this formula and I'm done. So I haven't had to learn the ": [
            2121.8,
            2152.8,
            49
        ],
        "vector and that's because just to make that a little clearer. What's a x to the X A- XP? okay, so here is the line here is like Where we are if we have x a and x b and we treat them at their points on this line, and we treat them as vector. Then x a minus XP. Is this Factor? So that's in the same orientation of ": [
            1313.4,
            1356.1,
            29
        ],
        "we take derivatives of the perceptron we get 0 and 0 is the slope is 0 everywhere with this. We actually can get it slow. So that's good. Okay. So yeah, so the ideas if it was to D like the or function over here it would be like this. It's still a linear separator because the hill is the same everywhere. It's still aligned, but it's got this ramp ": [
            1592.1,
            1629.9,
            35
        ],
        "weights as set the way. Yeah. Says what? how to set simplify to what so so yours be over at right if i r a z e to the log of that natural log of that then I get B over a dudududu dudududu, okay. here Oh, I didn't I just plugged it in so that was a a and that was be so I didn't want to write that ": [
            2152.8,
            2213.2,
            50
        ],
        "what that is, you should have taken linear algebra now and find out but that's life in the big city. Okay? Alright, so yes, that's what I mean. You invert a matrix that's closed form. This the closed form solution. There's no like You know, it's this times this and I'm done right in the linear regression case. This is a big vector and this is a pseudo inverse of ": [
            3126.9,
            3161.2,
            77
        ],
        "what we were just looking at was a linear discriminant on that side, but here's an example of a linear discriminant. You have some line and everything over here will call category to anything over here will call category 1 and write and And the way we do that is we have some function for category 1 some function for category 2 and if one is bigger than the other ": [
            221.8,
            253.5,
            2
        ],
        "why I want to drop those in so I don't have to draw them cuz I'm lazy. Okay. All right, everybody with me so far when I get all done. I can put it back in here and put little ends there and everybody would be happy. Okay. So what is that? That's - t - y. Times two cuz I brought the two down and I've got that too ": [
            3566.1,
            3602.7,
            89
        ],
        "with this to everywhere. It's really annoying. And I got some some. So this is the bachelor enrollware. I go through all the examples and this is mean squared error and I some all the way changes up. This is away change and this again is the Delta role because we call this Delta the difference between what you didn't want you should have done. And that's bad. She go ": [
            4210.0,
            4243.5,
            106
        ],
        "would just stick regression. So the whole idea here is so in a perceptron You have a damn it. Okay to take one of these over here and one over here. Okay, so remember perceptron? Is like that? And so it's zero. Everything over here evaluates to zero when we put it into the perceptron everything over here evaluates to one when we plug it into the perceptron. Now we're ": [
            1456.6,
            1499.8,
            32
        ],
        "you can solve these regression Problems by using simple linear algebra tricks that it's called the pseudo inverse. Yeah, okay. And so, you know I can so it's there's too many points for the unknowns. But in the end you'll get like the average height of all those points were that or that or that or that here? I'm going to have nine equations in nine and so I can ": [
            633.2,
            668.0,
            13
        ],
        "you. Okay, so this because you can keep doing that. As long as your hand doesn't get too hot. We got T minus y x x i. Do do do do do do do do so. That's that that's the Delta roll, right? That's the W equals wi + t - y x the input on that line. Okay question or is that just a comment? Because he's a constant ": [
            3804.6,
            3854.2,
            95
        ],
        "zero where why is just taking the input and multiplying at times the way it's doing the inner product? Okay. So what is that? Well, it's minus the Sun from n goes from 1 to Big end of the partial derivative of 10 - why'n quantity squared over weather spec to Wi and I'm going to stick a two down there. I just moved the two in so the sum ": [
            3490.0,
            3534.0,
            87
        ]
    },
    "File Name": "Deep Learning - C00 - Cottrell, Garrison W - Fall 2018-lecture_3.flac",
    "Full Transcript": "Listen to a podcast. Okay, let's get started.  check  Okay, so this is the first lecture and from what I understand nobody understood. Is that is that right?  Okay, so I just wanted to like point of few things out.  Okay, so  At first we took one feature of A's and B's and we figured out. Okay, so this is the feature and we're going to put a threshold somewhere here in anything less than that will call category one is in greater than that will call Category 2.  Okay.  This now, let's say we get a second feature. Okay.  And now all of the category 2 or most of it is over here and category one is over here this and we put a line between them.  That's a linear discriminant. Somebody said what's a linear discriminant? I meant what we were just looking at was a linear discriminant on that side, but here's an example of a linear discriminant. You have some line and everything over here will call category to anything over here will call category 1 and write and  And the way we do that is we have some function for category 1 some function for category 2 and if one is bigger than the other then we call a category that category.  Okay, any questions so far because you got if nobody understood it then we have to like stop. Someone said I could just go slower. So I think I might talk. Okay.  Okay. So here's the second thing people didn't understand. This is  a regression example where you're trying to get as close to some data as possible. Here is some glue dots that are generated from the screen curve by adding and adding some normally distributed noise. You just have a gaussian. You don't go to conferences and call it a normal distribution with say a gaussian distribution with 0 mean so we just add little bits to the green line and we get that data. This is what we're going to try and fit to the data, which is the polynomial of degree m.  Where which things are the parameters?  The wcso lot of people might think X. It's a variable so must be the parameters no exes the data. So, you know, we're going to get one point where X is here and tea. So this is x t is the target is here and what we do then is we that's point eight or something. We plug-in .84 X everywhere and we  Her Trying to minimize the difference between this and tea and we'll get an equation in m + 1 unknowns.  Okay, the unknown to the W's now we take a second example, we got an equation and plus one in their rooms. Every Point here generates an equation. That's a polynomial. Where was a polynomial until we plugged in and now it's just like w 0 + W 1 x Point whatever it is plus W210 whatever it is squared Etc. So it's it's all constants except the W's  Yeah.  So let me just stop for a second and say in some sense the XX Square... X to the m are the features this time, right? So we just take the features and takacs and we Square it we cubed cornet get whatever. Does that make sense?  The linear combinations you can put sweet.  Yeah, so this is a linear combination of the inputs, but it's like the inputs to different powers.  And since and we need something we're trying to minimize.  Until we get we start with this as if they were trying to minimize as some squared air and it doesn't matter what consonants are out here or even if it's 1 / n which makes it mean squared error.  This formula will have the same minimum no matter whether we if we * 3 where its minimum in terms of the parameters will be the same place.  Birthday doesn't matter if it's here or it's up here or it's down here. It's all the minimums always in the same place.  So to minimize that thing which is going to be a parabola.  Write this squared. It's a parabola in w.  okay, so if I multiply this out the teaser constants  the W switcher part of the formula are going to get multiplied times each other and I get w0w 0 squared etcetera. So it's a it's a function. I'm trying to get minimized and it's a parabola in the parameters.  Okay, so to minimize that I take the derivative of this guy and Saturday equal to zero. So the two is going to come down. I'll get y - t and then some more stuff depending on what why is  In this yeah.  So when I plot this red line.  So, okay. So what I do is I take the derivative here. I set it to zero and then I plug in All My Ex's so I'm going to get one equation for the first X.  Annatto, that should be X soup in one equation for the second X1 equation for the third acts. I'll have an equations in whatever number of unknowns my W's are so if I have you know a linear thing I'll have w0 in WW1. So I'll have an equations in two unknowns.  That's too many equations. But there's a linear algebra trick that still will minimize it so you can solve these regression Problems by using simple linear algebra tricks that it's called the pseudo inverse.  Yeah, okay. And so, you know I can so it's there's too many points for the unknowns. But in the end you'll get like the average height of all those points were that or that or that or that here? I'm going to have nine equations in nine and so I can fit the data perfectly.  1 2 3 4  Yeah, solve 9 equations and nine unknown so I can solve those using pseudoinverse and I'll be done.  Follow all of that. This is just an example in order to get to.  You know the idea of overfitting we're not going to do any polynomial fitting ever.  In this class. Okay weird. It's just an example.  Okay.....  Somebody asked what's Lambda Lambda just says how much we care. So one approaches to get more data and then I've got so much to say to the polynomial has to come as close to it as possible.  But the other approach to making the model fit the data better is to have something else we minimized. What is this do if we want to minimize this were trying to shorten the weight Vector which the weight Vector is the multipliers on the polynomial right terms. Lambda says how much we care about it if land has zero will have no regularization.  If Lambda is 1 then we're going to try and balance these two to minimize them both.  And if Lambda is 0000015 then we mostly care about this but we also want to make the weight Vector shorter. Why is making the weight Vector shorter Occam's razor?  Yeah.  You don't need less weights. You need smaller weights. So it would stay would fit in a little box as opposed to Big Box. Okay, so that's what I'm talking about here.  And we're going to you're going to do that in your homework your programming assignment. I think do we have that in the programming assignment? I don't remember. Okay play here lambdas, just right here. It's too big essentially and this is the case where it's 1/2 the year and 1/2 - minimizing thing and it just spends too much time minimizing minimizing W and not minimizing the air and and so this is with no regularization. That's too low.  Here's with too much regularization. That's too high. Here is the soup. That's just the right heat for Goldilocks.  Okay.  So that's what that was any questions about that now.  Okay.  kfin  just happened.  Okay, let's see if I can remember what the confusions were here.  That is Frank rosenblatt on the left.  Okay, so why?  why so a lot of people had a question about  What's going on?  Okay, what's what is this diagram, you know WTF Okay, so  Does not do anything, okay?  Okay. So what's going on here? We we have two dimensions.  Okay.  Okay.  Okay part of your homework. Okay, we have this thing where y of x equals 0 that's but y of x  is equal to  w  wtx for WW1 X1 + W2 X2. That's the inner product of the weight factor and hacks and I'm leaving out the bias here. And so that's equal to zero.  Midwives XC that's where where this is zero.  If it's a if the exes are a little if the Sun is a little bit bigger will say it's category 1 if it's a little bit smaller will say it's Category 2. And so this is where the line is. It's where Wyatt of x equals 0. So this is a linear separator because there's a line here.  And the weight Factor now in this setup is two dimensional the data is two-dimensional X1 and X2. And so if this is the equation of y f x equals is 0  We're going to call everything in the dataspace over here category 1 and that's Category 2 and the point of this slide is to show that the show this is a linear discriminant.  The W says where the separating line is, it's always at right angles to it and the bias tells us we're along that where that line is along that Vector. So if we were doing or and this was 01, and this was one zero and that's zero zero and this is one one. We want the network to the system return one. If all of those are on this side of the plane or the line and zero, otherwise if we are doing and the bias would be different in this line would be over here. Just cutting off one one.  Okay.  And given the solution we found like one 1-1 it would be exactly at 45 degrees here.  Right and -1 would make this one over that.  What should be the square root of 2?  Okay.  Any questions now?  What is a W-9?  Why is W not have to be negative all the data's over here? I guess this W or this W. This has to be negative because it's a distance. So minus a minus is a plus if the weight Vector was pointing over here there wouldn't be the solution at BW 0 instead.  Joe  there'd be no minus sign in front cuz it's a distance. So this confused the students last year and then I realized well this example they're assuming it points this way and that turns out to make W negative which means minus minus is a plus and we're okay. It's a distance.  Okay.  Alright any other questions?  Yes.  So the bias is always the option of threshold. So w 0 equals minus Theta.  But we we made Theta go away earlier because it made makes the learning roll simpler to describe. So those are elected last year's slides data in there and I had to say off we're on and we're supposed to be off and I want to raise the weights and lower the threshold for off and we're supposed to be on I want to raise the weights for were the threshold. I think I said the same thing twice, but were the bias. I just say, I raise the weights in the bias or a lower the weights in the bias. So it's few or characters on the slide.  Okay.  An end here I'm just showing that the separating line. That's what you had to do in your homework. You had to make this into slope intercept form MX plus b and I didn't drive it because he had to do it.  And this is just a proof that this line is perpendicular to the weight vector and that's because just to make that a little clearer. What's a x to the X A- XP?  okay, so here is the line here is like  Where we are if we have x a and x b and we treat them at their points on this line, and we treat them as vector.  Then x a minus XP. Is this Factor?  So that's in the same orientation of the line. It's a line segment and what I prove there was that w?  W x that the inner product of those two is zero, and so they're at right angles.  Okay.  Okay. Alright, so.  Is anybody still confused?  Good.  So what a perceptron is a linear separator it put linear because the line and it can only separate things where there is a line between them. So that's what it can compute only linearly separable things. And if it was 3D then we have a plane and if it's 4D we have a hyperplane & Beyond hyper playing all the way.  All the way down or up.  Okay.  butcher to okay  Witcher 2  since we're since I program and are used to program back when I programmed and see we count from zero cuz we're computer scientist.  Okay.  So would just stick regression. So the whole idea here is  so in a perceptron  You have a damn it. Okay to take one of these over here and one over here.  Okay, so remember perceptron?  Is like that?  And so it's zero.  Everything over here evaluates to zero when we put it into the perceptron everything over here evaluates to one when we plug it into the perceptron. Now we're going to do logistic regression where y of x a where a equals the weight of town of the inputs from equals 0 2D of WI  that's a  and why has some I'm just writing this because we call this the activation function for perceptron. It's that weird graph for logistic regression. Its 1/1 + e to the minus a  And that's shaped like this. It's called the squash and function cuz it takes the line and squashes it down. It's also called the sigmoid which is Greek for ass basically or S. Like if you tell it your ad this looks like an s  Okay.  Sure, it's okay. That's the activation function and it's nice and smooth instead of the perceptron which isn't smooth. It's got a discontinuity and since we're going to use calculus and take derivatives if we take derivatives of the perceptron we get 0 and 0 is the slope is 0 everywhere with this. We actually can get it slow. So that's good.  Okay. So yeah, so the ideas if it was to D like the or function over here it would be like this.  It's still a linear separator because the hill is the same everywhere. It's still aligned, but it's got this ramp Behavior.  Hey.  All right.  so  so we have this monotonic activation function and here again, I'm separating out the bias.  So it's the inner product between the weights and the input plus this bias term and G is the sigmoid.  Yeah, so okay, you can also Imagine the same network being linear regression Network where G would be the identity function.  And then were were actually plotting a line in the space to get as close to the data as possible.  Okay, so we're going to give this counterintuitive. We're going to make this hard by giving this counterintuitive derivation. So that are two categories are gaussian. So  What you should have in your head is that this is category 1.  And like this is Category 2.  So maybe this is some feature.  And these are the A's and these are the bees.  Okay, just imagine that's what happens right? So this could be  you know the value of the feature in a few remember and go back to the first lecture where he had histograms their kind of gassy and ish just like I'm vegan ish because  I eat pizza. So this could be your midterm grade. Right? And this is you guys and this is some monkeys taking the class. So that's really easy to separate that's less. So so we're going to have the separated with a gaussian over with a logistic function.  And what it's going to do is it's going to be low over here.  And it's going to start to go up and reach about a half here because it's 50-50 whether you're in category or category B, and then it gets higher over here and it's a high probability of being a b.  Okay, so that's  and so  What this says up here. I used to have all blue slides with people said that hurt their heads. So I didn't get all the way to switching this to these are just cut out from the other side. So the probability of an x given that you're in C1. So here's C1. It's this distribution. These are the probabilities of being an axe have a high probability of being here low-probability being here in here. That's the probability density function of the sea ones and this is the probability density function of the sea to say it's that one  And that's just the formula for a gassy and the only difference between them is the mean new one here Mewtwo here. So me one is right here. Mewtwo is right there.  Okay using Bayes rule to figure out okay, this is the probably the data given the category, but what we really want is probably the category given X.  And so using Bayes rule, if you don't remember Bayes rule go look it up. I guess it's a probably x given C1 times the probability of C1 over the probability of the data.  okay, then probably see to is that so now if we know the prior probability of one category in the prior, probably the other and usually we just  They're just one half but you can have data that's skewed in some way by there's a lot more of one category than the other and this the probability of the data. Then it turns out since these two some to one.  That means that this plus this has to some to one so we can put that plus the numerator.  Sum that up on the bottom and that normalizes it.  2B, you know, we don't have to actually no P of x.  Since we know these some of the one there's only two categories then that means that the denominator has to be this normalizing constant.  Okay.  Okay.  okay, so now we've got a formula for weather what the probability of it being in category 1 is so if I said call that a happy then I can divide through by a  okay, then I can.  take this and say oh, this is the e to the log and be over a which is just be over a  Hi and then  I can say oh no, its 1/1 + e to the negative log of A over B, which is just a minus and field. This is still a / B / B / a and then  I can plug what I've got back in.  This guy for one and that guy for the other in the key of X is canceled. And if I call this whole thing, I've got the signal.  In fact, you can take anything and turn it into the sigmoid pretty much following along. But anyway now I've got a logistic regression.  categorizer  So as that thing.  Which was this thing?  applied at the logistic already another words to probably class one follows a sigmoid as a function of the log ratio of the probability of class seated press that is the law God's basically cuz these are going to cancel because they're usually equal  So that's a kind of motivation for the sigmoid.  Yeah, cuz otherwise, she'll Hell Breaks Loose.  It's not so simple is this.  So in other words, we can interpret the output of the sigmoid as a probability the posterior probability. It's called in spaceland.  It's probably a category 1 given X and then that thing can be written as something that looks like.  The weighted sum of the input supposed to buy us if I set the weighted sum of the end. If I said wa equal to that end W 0 equal to this crazy thing and so I can interpret it as I can just read those. I can just look at the parameters of those two gaussians and set the weights.  Right away. I just plug into this formula and I'm done. So I haven't had to learn the weights as set the way. Yeah.  Says what?  how to set simplify to what  so so yours be over at  right if i r a z e to the log of that natural log of that then I get B over a  dudududu dudududu, okay.  here  Oh, I didn't I just plugged it in so that was a a and that was be so I didn't want to write that all over the place.  right  So I just plugged it back in.  And a is p of x given see one P2P of c and 1 and over this and B is probably a C2.  Blah blah blah blah blah, okay.  So I just was simplified it by calling this thing a / A + B and then when I got here, I plugged those things back in for a and b.  Yeah, okay, who's at the desk that okay.  Okay.  Okay, so that's really nice, but we can't assume that our data is going to be gaussian.  If you know if we happen to know they're gassing they have this mean in the same variance, then I can just set the weights and I'm done. I don't have to train a neural network at all. But we don't know our date is gaussian the most things are gaussian, but you know almost everything is calcium in the world. We need to learn weights, but we're still going to interpret the output is the probability of category 1 given the input.  So what do we do?  What do we do?  Tell me you know what we do.  Gradient descent. Thank you. Sorry you got the slides from last. Okay, and as I've been used to learn machine translation from English to French French to English Setter Etc has been used for playing training a system to play Go Fish things are going to drive your car soon later in the quarter will see how you can even have a system learn to program learning program to recognize faces and emotions object recognition image captioning generating new images of faces that scans or generative adversarial networks, which I finally did a lecture on last.  For the crab students. I'll probably give it to you too.  So what is gradient descent you have some objective function something that you're trying to minimize. How about minimizing errors seems like a good idea. That's what you want to do on your midterm. Right? You might as well minimize the square there right the same going to be the same thing. So we want smaller. So we have this expression that we did with the polynomials the sum squared are we want to make it smaller? So we'd like to change our parameters in such a way to make the arrow smaller.  And the idea is to take the derivative of the sum squared error with respect to one of our parameters.  And you assume all the other parameters are constant that gives you partial derivatives turns out that's the only difference and that tells us which way is up in the air.  But we want to go downhill. So we take the negative of the slope and add that to the parameter. Here's the idea. You should have in your head.  at least four  Like I said, here's some parameter wi and we imagine all the other parameters are fixed it whatever their value is right now so I can plug wi into the formula for sum squared error and for some values of wi it'll be high and for some values of wi nobilo. So this is SSE and in fact because it's some squared error. It'll be a parabola.  In terms of this parameter. So this this is the sum squared error as a function of wi to function no place where it has two values. Okay. So suppose we start out here.  What I want to do is compute the slope of this thing that tells me which way will increase the Earth. So I take the negative of the slope and that's just some number, you know, it's the rise over the Run.  Right and that's going to be some number and I subtract that number from wi and I get here now. I'm here figure out the slope figure out the negative of the soap by multiplying by -1 surprise surprise, and then I subtract that from W and at some point the subs going to get smaller and smaller and hopefully will converge to wear to minimum.  That's gradient descent. So now we plug that into the substrate are and we'll get the minimum sum squared error and but we do this for all of the parameters at the same time.  And that's what we do now usually this.  Distance that we go is is the soap and it usually is love going to be really big up here. So you have a learning rate. That's how much of this we do.  And so if the learning rate is too small like I showed you last time you don't get there very fast. If the learning rate is too big. I might actually jump over here and increase the air.  And that's what happened when I set the learning rate to to on that perceptron thing in it. And the separating line just left the left the stage. Yeah.  Yeah, so you pick some value like or no point one or .01. You don't want something bigger than one cuz then you're like  Really going across that's why I said it to 2 and it went crazy. And typically though. What you're really doing is is Anil the learning rate, which is what I did by hand and that simulation it kept making it smaller and smaller than the line stop wiggling so much. That's because I'm just going back and forth over here. If you're learning rate is higher than that. You'll get down to some point where you're jumping across here and it stays High.  I know if he then lowers learning right and I'll go there if you keep lowering learning rate. I'll jump back and forth over here and then  Okay, yeah.  No, not really. But this is the nice thing about Square there is there's a global to local minimum in the global minimum are the same once we get into neural networks with multiple layers are going to look so pretty if you think that looks pretty it'll be like this and you're going downhill and you can end up here when the best answer is over here, but the when happens is that most of the time a local minimum is good enough.  For government work. Yeah, yeah.  I am I doing this Saturday of layover every yeah, that's bad. If the features are correlated. We'll talk about that and a couple weeks. But yeah on every step I go through I can go through all the patterns. That's Bachelor ending figure out which way is the add all those little weight changes up.  And so you go down. So the sum squared error is the sum of the squared error over every pattern. So you have to do this forever and Eagles won to begin.  And you add all those up?  And then you go downhill or you can take one pattern one example and go downhill just based on that and because you're only looking at one example, it's not this, you know, this is the sum of many parabolas for one for each example. So if I just take one example and go downhill for that, it's not necessarily the same as the direction that I would get if I went through all the examples, so  That's called stochastic gradient descent because you randomize the order the pattern so you get random examples and they go this way this way this way and on average they tend to go down the right direction.  Doesn't matter actually so I can put one over N and is fixed for my training set. You know, maybe I've got a hundred example, so it's one over a hundred. That's the average if the minimum is going to be in exactly the same place.  I don't remember what I told you to do in the assignment, but whatever I said, that's what you do.  So, you know by scaling, you know, if you have one over and then the sum is a lot smaller. It's the average not the total sum squared error. And now if I change the learning rate I can account for that.  So cuz it's going to scale the weight changes.  Okay.  Okay, so I haven't told you this but linear regression is when you're trying to sell it turns out I'm going to the next couple of lectures are going to be magical because and also really boring because I'm going to show that the perceptron rule the Delta roll for the perceptron is going to be the same rule that you want. If you're doing linear regression that's going to be the same rule that you want. If you're doing logistic regression and it's going to be the same role that you want. If you're doing multinomial regression softmax regression, so  Do do do do I'm so that's the theme song to The Twilight Zone by the way, you're too young for that.  Don't watch it because parental advisory. But anyway, so they're pretty good. But do you want the Twilight Zone was Rod Serling introducing it?  Anyway, you can probably find it on Netflix or somewhere. I'm not sure but it's it's a it's great nothing like it but the theme song is do do do do do do do do and that looks like  Who are something weird is going here in the Twilight Zone?  Okay, so  I didn't tell you this but linear regression you're trying to draw a line through some data, you know if you've got some points.  And you're trying to find a line that's as close to all the points as possible. And it turns out there's a closed form solution for this. He's been just invert a matrix or convert use a pseudo inverse and boom you're done and that's why you can call a Matlab function for linear regression and boom you're done. There's no gradient descent, but you can do gradient descent and it turns out with linear regression does is minimizes the sum squared error. That means Burger.  You know, it's like a ax equals be right if I want to know what exit is I set x equal to be?  x a 2-1  Okay now replace all these with matrices.  It's a big Matrix. She's you know.  Yeah, I mean like this is W, right and this is your parameters and you can have lots of parameters and next is going to be your design Matrix with all the examples in this is going to be a vector not just be but a whole Vector of the targets for all those examples and then it turns out but it's not a square Matrix because you have more examples than parameters usually and then you have to do a pseudo inverse, which if you don't know what that is, you should have taken linear algebra now and find out but that's life in the big city. Okay?  Alright, so yes, that's what I mean. You invert a matrix that's closed form. This the closed form solution. There's no like  You know, it's this times this and I'm done right in the linear regression case. This is a big vector and this is a pseudo inverse of a and which has a big Matrix and and these are the W's  Oh, actually it should be the other way around yet. So let's say w x equals the targets. Okay now to get what w is I have the targets X the inverse of x.  Well, when all these are matrices and X is not Square its rectangular then you do the pseudo inverse of X and that's provably minimizes the squared error. So I just have to invert a matrix.  Now that actually can be iterative to do that. But yeah, that's all you have to do.  make sense  are we happy? Cuz cuz I'm never going to make you invert a matrix. If you have to invert a matrix, you should just go home and crawl into bed and cover your head with the covers. He's really don't want to invert a matrix. Okay, so linear regression is saying I want to find this line.  sell my model y equals c n o w x  for all the EX's and all the the target. So I'm you know, I want tea and equals a w x n for lots of ends.  right, and so  And I do that by taking the sum squared error for the mean squared error or whatever and I put a 1/2 in front to make things simple the mean Square. So I need an objective function just like for the polynomials. So I take n equals 1 to Big end of all the examples TN - y n  squared  okay, and that's what I want to minimize.  if I want to do it by gradient descent  Why has some W's in it, you know, let's let's assume that so y a x  Is xn his WT?  Xn Rachel hear X is a vector W is a vector and that's that's our model of the data turns out to be a liner a plane or hyperplane etcetera and we want to get it as close to the data as possible. We're not doing classification. We're doing regression here. We're trying to fit a line to the data.  so  what are we do?  We want to minimize this and we have all these parameters.  and the update rule for gradient that says  wi is the old wi so this is one of our parameters - the partial derivative of the SSE.  Weather expected that parameter. That's exactly what I've been saying all on this is the slope with respect to that one parameter. We figure out what that is and then we going the opposite direction. So this is that little change of w that's making the error smaller. This is the formula for gradient descent doesn't matter what whether we're talking about W or some other model some huge complicated model with millions of these guys. We still are doing this.  We're going downhill in the air. So how do we do that? We need to figure out this guy.  Right, and that's the thing. We're going to add to W to make the error go down, right?  So we want to know.  What is -2 partial derivative?  Of the air with respect to Wi, what is that?  Okay, and the rest of the slides in this the side deck do this, but I'm doing it more slowly so you can follow me instead of like going  Okay, so we want - so minus the partial derivative of the son of 1/2. I'm just hummus and goes from 1 to Big end of tea and mitos Y and quantity squared that's our objective function that we're trying to go down Helen Wright.  And here by the way.  This is our model.  Now we're not having a yes or no or one or zero where why is just taking the input and multiplying at times the way it's doing the inner product?  Okay.  So what is that? Well, it's minus the Sun from n goes from 1 to Big end of the partial derivative of 10 - why'n  quantity squared  over  weather spec to Wi and I'm going to stick a two down there. I just moved the two in so the sum the derivative of a son that is the sum of the derivatives derivatives are linear writing linear operation so I can always do this and now in order to make my life easier.  I'm just going to figure out one of these derivatives, you know, therefore men goes from 1 to n and I just found them up. So I want to just figure out what this is T minus why I want to drop those in so I don't have to draw them cuz I'm lazy.  Okay.  All right, everybody with me so far when I get all done. I can put it back in here and put little ends there and everybody would be happy. Okay. So what is that?  That's - t - y.  Times two cuz I brought the two down and I've got that too in the denominator WI.  And that's why I put the 1/2 there is the derivative of T minus y.  I'm sorry. I don't need this year Wy.  Already, it's starting to look kind of like the the Delta Road got that Delta there.  So all I did was use the chain Rule and take the river this thing and then I take the derivative of the thing inside of it.  Okay.  Okay, I have to be able to remember what I wrote over there when I get over here, but instead.  What can I do?  Okay. Now I don't have to remember what I said. Ok - t - y.  time's the derivative of e  tea with respect to y s r e t respected WI  Minus the derivative of y with respect to Wi, okay, the derivative of the sum or the difference is the derivative of the is the sum of the derivatives. I got - t - y  This is a constant. It's given to us by God and the training set.  So, this is Zera.  And now I've got minus the derivative of y with respect to Wi.  Okay, and now I'm just going to get rid of those minus signs cuz there's two of them for a bit of Y with respect to Wi.  Okay, and now what's why why is the inner product of the weights for the inputs?  So that becomes T-minus why is the derivative of the Sun as I goes from 0 to D of wixi with respect to Wi oops, I better use a different variable. How about Jay?  Jay  Jay okay. So what's the story of it is?  X i y because we're assuming that all the other W's are constants when we're doing a partial derivative. And so and this is constant because it's given to us in the training set. And so the only part of this song that's not zero is the one where Jay equals I and so we have the derivative wixii over wi which is just x i  Oh, you're you're talking. Thank you.  Okay, so this because you can keep doing that.  As long as your hand doesn't get too hot.  We got T minus y x x i.  Do do do do do do do do so. That's that that's the Delta roll, right? That's the W equals wi + t - y x the input on that line.  Okay question or is that just a comment?  Because he's a constant and you know, unless your Jimi Hendrix. You don't think you can change constants. You know, he said if 6 turns out to be 9, I don't mind.  Cuz you got your own world to live in and it ain't going to bother me.  He is the Target tea for Target tea for teacher.  bike xr100  Okay, so we don't want to change any constants because then everything will go to hell.  Okay.  Okay, so that was linear regression.  But in this case for linear regression T, you know, all your examples are going to be these points you're trying to fit and so particular X.  This is T. It's not one or zero because it's linear regression for this axe. It's this tea Etc. So these RX tea pairs.  Input output desired output and because we're using the squared error, which if you take the square root you get euclidean distance. We're going to find the line that's closest to all the data.  Okay. So this is a weird way to do linear regression if we got a closed form formula. Why would we ever want to do this?  any ideas  computational say what?  Why does it?  Yeah, so if this is the year of a big data or jucunda data, whatever and so if I have a matrix this big and I have to invert it. It might not even fit in memory.  But if I can take some examples and change the weights for every example using the Delta rule, then I could do it iterative lie, and I won't run out of memory.  I'm not trying to invert a big Matrix again. You don't want to have to invert a matrix if you can avoid it.  Okay.  Hey other questions.  Oh, it's it's the exactly the same right? I didn't I didn't say anything about the bias. We got to do something else know the bias is awake from units. It's always one.  And so w 0 equals the old W 0 + t - y.  * 1  so this is why the bias off and changes more quickly in a neural net because X in the neural net is usually some number between 0 and 1 or 1 or -1 and so it's it's not always a very big number. Where is the bias will change quickly depending on the difference between the Target in the inputs. Yeah. Is that a question or  Yeah, yeah. So again celebrity rate. Yeah, this is is essentially corresponds to learning rate of 1. Let's go back to  So usually you speak in an alpha here.  And this is the Robin's Monroe procedure from the 50s and they proved some things about what the learning rate oughta be. And how how are you should make it smaller and smaller and she go in order to ensure convergence.  So they are there three. It's got to be a certain big bigness but it can't be too small. It's got to be just right and you have to change it as certain rate the rate you change the learning rate. Is he start out with some learning right? Say alphazero and then you go through all the data you change the weight.  Then you take Alpha 0/1 that doesn't change much. Then you go through the data again. The second time that's called an epic when you go through all the data and now I take that initial learning right in / 2 then so if you use Big T to stand for how many iterations you're doing you do that?  How do you know if I'm on the teeth time? I'm changing the all the weights I / T that turns out to be too too aggressive and in fact, so that's the theoretical where you have to prove it. Most people use something like the square root of that. It makes it smaller and doesn't make the learning rates us too small too fast.  Okay.  Okay.  So there's all this stuff in here you can look at.  And that's what I just did mostly accepting this version. I forgot to put the 1/2 in front and I ended up with this to everywhere. It's really annoying.  And I got some some.  So this is the bachelor enrollware.  I go through all the examples and this is mean squared error and I some all the way changes up. This is away change and this again is the Delta role because we call this Delta the difference between what you didn't want you should have done.  And that's bad. She go through all the examples compute the way change, but you don't change the weights.  Here's to another one change the way it's cuz I don't change the way she just some of them all up and then you take the average and change the way it's once this is typically a bad idea because a lot of times the data set is redundant in a while. So everybody's happy face looks kind of the same because you got this big smile, if you change the way it's as you go you may learn more by the end because you got a lot of redundancy in the training set.  So that would be online learning where you got an example and you actually change the wait for that example. That's stochastic gradient descent.  And so again, it's the Delta rule, okay.  So there's a generalization to perceptron called logistic regression at places where places that buy that which allows us to go from zero one class for class first gives us a probability of category membership.  And unlike linear regression for logistic regression. There's no closed form formula anymore.  It's not linear equations anymore. And so we have to use gradient descent.  Okay, and I done traded gradient descent for linear regression, which can be useful for Big Data.  Minimizing mean squared error leads to Delta rule for linear regression. But look I predicted. I was going to run out of time before I can show you how that generalizes the logistic regression, but that's your homework. Anyway, so in your homework,  We don't use some squared error because when you do that with logistic regression you get a learning roll. It doesn't work as well as if you minimize a different objective function, which is cross entropy.  And so when instead of some squared are you try and minimize cross entropy?  You got this.  Dudududu dinner. Okay, and so but that's your homework is to try to take that derivative. And what are you going to have to do?  well when you get to the part where  When you get so I've already done most of their your homework for you here.  But when you get to the part where you got this?  10 - y + x the derivative of y with respect to Wi now I've got to  why is no longer just to wait and some of the inputs it's the logistic function. And so we have to take the derivative of that and then take the derivative of the weighted sum by using the chain rule. So derivative of y if x was inspected wi is y Prime of x  X the derivative of x with respect to W. I wear this is the weighted some of your inputs.  But if if everything works out just right that'll cancel out with something else and you got the chain rule you get you get the Delta roll.  Okay.  All right, and I also showed although I didn't really show you that that guy and that guy.  Is that Jeff Henry on the Coon?  Okay.  I didn't really show that but you can tell you can do face recognition really well and you can tell those aren't the same.  Okay, so that's it for today.  No problem.  Are non-linear equations?  You can try.  Are we going to talk about how?  Welcome to Facebook.  Yeah, that's me. Some of the coolest stuff when when it's like holyshit now.  I was reading up on there like while ago and I'm going to teach about it, right?  You will be jumping the gun as they say that's all right.  I saw that we are using L2 Norm or  UC San Diego podcast "
}