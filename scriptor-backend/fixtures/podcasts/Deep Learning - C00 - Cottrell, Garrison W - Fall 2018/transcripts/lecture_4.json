{
    "Blurbs": {
        "- IG Prime Of a the slope of g at that point x the derivative of a with respect to Wi. Okay, and what is AA again is the weighted sum of the inputs? so this is the derivative of the Sun from I or sorry jay equals 1/2 D of w i x i s r e j j j j was expected ww-ii Ask Sai why because when ": [
            1223.9,
            1276.9,
            23
        ],
        "Cross entropy. No problem, you know, this isn't here if we're really confident and wrong. We're going to get a big slope there. You got a big error signal? All right. So not knowing any better four years. This is what we did. This is what's in chapter 8 of the Old Testament that PDP book volume 1 and if it does work, I mean this did we use this ": [
            1619.1,
            1653.5,
            31
        ],
        "Delta rule. so any ideas There's nothing wrong with the math math is all good. For what could be a problem with this. What if Yeah, what if giave is already close to zero? So the output of the network is almost zero what happens to the learning? for that pattern You know, it's always G of a of N and that's going to be very close to zero in ": [
            1517.5,
            1583.0,
            29
        ],
        "He's got a bug somewhere. Okay, if you're still here, okay? So we can take it to logistic is that the softmax is actually a generalization of logistic. If you plug into the soft Max just two categories. You can turn that into a soft into a logistic. So it's a generalization to more than two categories. So we've seen four kinds of neural networks so far linear Network, which ": [
            1866.8,
            1908.5,
            38
        ],
        "I don't remember what beta is or what we're talking about here. But it's just I think it's just some picture I found on the internet and I wanted to use it because it makes the point well. So another words to probably the target given the data is his calcium. Okay, so here's the target. And HVAC Savannah is our model of the data. So this is our neural ": [
            3669.1,
            3703.7,
            83
        ],
        "I must have turned off and I see. Start new session, okay. Start new session. Okay. Where'd my little thing go? There it is. Okay. Mariah Carey Okay liquor is live. Hey, is it 59 of you 60 of answered? Okay. What's your answer? 76 of you've answered and oh, I haven't looked at the distribution of answers. Okay, good. Okay, it's more than 80% of you. That means I ": [
            3129.0,
            3195.4,
            72
        ],
        "J equal sign. and so that's t- why? x - G Prime of a time's the derivative of w i x is expected wi, which is just X. so Okay, so that okay. So there it is. That's what you get for learning rule. when you use mean squared error with the logistic function Okay, so no no, no, no. Okay, so all right Sue. Okay, so fun fact. That ": [
            1361.9,
            1437.6,
            26
        ],
        "Listen to a podcast. Okay. I'm going to start right off with a few clicker question. Sarah get your clickers out. and should be a a I haven't been able to find my Professor clicker so I don't can't change the channel. So I hope no one next door is using clickers on a okay. You ready? Set Go Oh, I have to say sorry. I haven't started it yet. ": [
            1.9,
            69.0,
            0
        ],
        "Okay, so here's the squashing function. And the slope is this * 1 - this that means it's going to be like that. It's sort of like a normal. Hey, so that's tea Prime. Arvak's enough T-Rex money maker Okay. So that's that's cool. We were happy about that. We didn't have to do much to calculate the slope. But what's wrong with this? It looks a lot like the ": [
            1471.4,
            1517.5,
            28
        ],
        "Okay, that's too bright. Okay. So here's the idea. Here's a regression problem. We have all these blue dots. That's the data. We've got we're trying to fit the data. But let's let's assume right now. Did this red line is the underlying deterministic function that generated this data? How did it generate this data by drawing from a gaussian with 0 mean at the value of the of the ": [
            3589.6,
            3629.1,
            81
        ],
        "Okay. All right. question 2 the following is an expression. No, I'm not going to oh, how do I find my friend there, please? Where's my mouse? This isn't showing up on my screen is my mouse. Okay. So this is the results of the maximum likelihood estimation for a 1D. Gaussian. So it's an expression for the results of mle. Okay. Oh, I haven't started. Sorry. I have to ": [
            3303.8,
            3361.9,
            75
        ],
        "Okay. There we go. Okay. Okay. The answer is not he. are you want to put it there, but Okay, is everybody voted for their only 65 of you? Really? Okay 66. Okay. I'm going to close it out. going going Going Sean. Okay, what's the answer? Thank you. 94% of you got that for four people. I didn't get it. Why is he answer not be? Yeah, this this ": [
            70.5,
            156.0,
            1
        ],
        "Red curve, so all these points were drawn from a gaussian year and now she in here guessing here guessing here etcetera. So we assume this so this is like noise. OK it could be measurement noise. It could be actual noise if you're in a bar. Okay. So this is the this thing represents the probably the target cuz it's Galaxy and distributed x 0. Given W and betta, ": [
            3630.8,
            3669.1,
            82
        ],
        "So again We assume that the likely it is the product of all the data points. Which is and this is the distribution. Why haven't the tea event 1-5 end in one month? So what if it why is in category one? So what's the probability that sin category 1 it's one because the 10 category one. Okay, so Reuse tea event equals one category one that makes y ven ": [
            4114.5,
            4154.2,
            94
        ],
        "This is the this is the schema for gradient descent. If you want to change your weights, you should change them in the direction of that will. Decrease the objective function. So this part says uphill in the objective function When You Subtract it you go downhill. Okay. This is the Delta. So I tried to fill you with this because this is delta T. Minus. Why is delta T ": [
            395.4,
            434.8,
            5
        ],
        "We get this on the top and this on the bottom, so it sums to one. So it gives you a probability distribution over the categories. And we'll be seeing the softmax again fairly often. So that's sometimes called a softmax distribution. So if anybody goes up somebody else goes down. It's like a winner-take-all network. So that's what we used to call it in the old days. She had ": [
            1799.4,
            1832.5,
            36
        ],
        "Well, that that would if it has a zero Target, then this becomes Iran this becomes one and we get this was 1-0. So this is the probability that 10 Category 2 that becomes one that becomes one. We just send up with that. Okay makes sense. That's pretty cool a t kind of gates those two things. Okay. So now this is the likelihood of the data. He believed ": [
            4184.2,
            4224.1,
            96
        ],
        "What's this one? That's just any anybody somebody what? I just you can leave it in terms of G. Where does it? Yeah. That's after we figure out what this is first, which is high. Okay, what's the derivative of f of x with respect to X? F Prime of X the derivative of x at that point right? So so we get so we get 3 - y x ": [
            1158.2,
            1223.9,
            22
        ],
        "What's this other thing? That's the constant out front came it became a since it was negative log likelihood. It came became a plus but we're not trying to fit that. So we can remove that term because it's constant with respect to the weights cuz we're not trying to fit Sigma. We're trying to fit this guy. And this Factor doesn't do anything either. It's just me and we ": [
            3881.4,
            3914.6,
            89
        ],
        "a lot of mistakes people made early on your friend of mine still does this is a fine activation function to the input switch, you know, the logistic it's just kind of weird, but you don't do that. So is it just the the Ryan puts that and then this is where we learn so These could be rather they could be Logistics. They could be 10 H. And typically ": [
            2227.1,
            2258.7,
            48
        ],
        "a network with multiple answers and they were all inhibiting one another the guy with the most input usually would win. So that's how we make decisions in their own that send you make a decision here by picking the maximum output right say I had a student in my office hour today saying he was getting 100% on the 6-way one. Anybody getting that. Okay. That's a bug whoever-you-are. ": [
            1832.5,
            1866.8,
            37
        ],
        "about this is if you've got it as a hidden unit is that the slopes really easy one year and zero here. There's like nothing to do basically, right? The weird thing about it in terms of like a neural network is real neurons. Don't just keep going as high as they can go. They have a maximum firing rate. So that's one. There's also one called leaky relu where ": [
            2093.7,
            2130.1,
            44
        ],
        "all the data points in the in the training set are independent so we can product them together. But this factors if you remember your probability in the probability of T given x times the probability of x. And taking the negative log, we get minus negative log likelihood is minus the sum. Of the log of this plus the log of this. Okay. Any questions on that? That's just ": [
            3509.3,
            3547.0,
            79
        ],
        "all the other categories turn to zero in the end up with just the guy for that category. Okay, so that's the probably one pattern. It's the product over all these wise. And most of them will turn to one with a one-out and coding except for the one that has the one so that's the probability distribution. That's a probably one pattern. So, you know, this is just illustrating ": [
            4388.5,
            4425.6,
            101
        ],
        "all the same. I'm trying to maximize the log likelihood. We're trying to minimize the negative log likelihood. We're trying to maximize the likelihood. D&C Let's see if it's this works down doesn't work if this works. Well, it stopped again. Okay. All right, so that's it for clickers for the moment. So now you can go. Okay. Okay, so Or finding a Galaxy and we assumed one dimensional data ": [
            3416.8,
            3474.6,
            77
        ],
        "and this Do-do-do-do do-do-do-do. So we get we get processed for multiple classes. so again enduring Seaway classification multiple multinomial regression if we assume the targets are multinomial a distributed processing. Okay, so in the next lecture, okay, so I told you all this in the next lecture. I'm going to talk about some other approaches to objective functions. These aren't the only ones. And then start with some tricks ": [
            4459.5,
            4504.5,
            103
        ],
        "and we usually just call that the air. So that's the likelihood that's the log likelihood. It's the negative log likelihood to come here. Okay. I already said that. So these are all equivalent finding the maximum of the log finding the minimum of the negative log fighting the minimum this thing. Find the minimum of this thing. I guess what we had a Galaxy in here. What's a gas ": [
            2969.3,
            3006.3,
            68
        ],
        "and z is bigger than y then the log of those are in the same order, right? It's a monotonic function doesn't change. What where the peak is. So often what we do is we stick a log in front of that. We are. The max of this then is the minimum of this with a negative sign in front. All right. That's all I did there. And there should ": [
            2903.3,
            2934.1,
            66
        ],
        "be some parentheses around this whole thing and not subtracting this from the argument. Okay, so the cool thing that happens when you take the log Is it turns into a sum and then you have the log of the probability of each point? Okay, any questions so far? So typically we call this the error. Right try to maximize the likelihood but instead we minimize the negative log likelihood ": [
            2934.1,
            2969.3,
            67
        ],
        "can assume the signals or some constant. and that doesn't affect the minimum. It's still going to be the same minimum whether this is here or not and I get Do-do-do-do do-do-do-do. sum squared error So do do do do do-do-do-do is the sound from Twilight Zone, and I'm going to say it a lot. so to recap when doing regression if we assume the targets are ghosting distributed. And ": [
            3914.6,
            3966.2,
            90
        ],
        "categories You know. one perceptron might shut off all the pictures of Gary and one might cut off all the pictures of Bob. One might cut off all the pictures of Ted. So they're all fighting it out. Okay. And perceptrons again can be considered a linear discriminant. All they do is put a line down in the input space and on one side of the line. They're on the ": [
            1986.0,
            2027.9,
            41
        ],
        "cuz T is a constant. and so You got th - y. X the derivative of y -2 driven is a y or w i but why she of a is a so I can just do this, right? No, I can't because this is logistic regression. So y equals g of a How much is 1/1 + 80 to the minus a okay, so I can't just get rid ": [
            1063.6,
            1110.2,
            20
        ],
        "data now? Okay, is it a Galaxy in? That doesn't make any sense here. Right? We don't think you know, this is Happy plus or minus some gassy and noise, right? It doesn't make sense. It's not the right distribution of the data right? Not the right likelihood form. So what do you think it is worth what distribution models are coin flip? Bernoulli Okay. And there it is. Okay. ": [
            4068.4,
            4114.5,
            93
        ],
        "don't get it still turn in your programming spent minutes and your homework, but that is a little tricky of a derivative. So don't feel bad. If you can't figure it out. Okay? Okay, so we're going to start today. We're going to start with me and squirt or not. So what we did in the homework was start with cross entropy. And you're going to see why we did ": [
            642.1,
            674.3,
            10
        ],
        "don't have to go to talking to your neighbor. That's that's what I learn from Christine Alvarado. Okay. Okay. Now, it's 70. Okay going to close it out going going going gone evokes. What's the answer class? See, okay good. So a few people voted for d And at least four people voted for a so maximum likelihood maximizes the probability of the data. Given, you know you trying to ": [
            3195.4,
            3247.4,
            73
        ],
        "equal to one is an injury or otherwise, so that's one hot encoding. How do we write the likelihood? Okay. Well we can just put it up here the TV up here, right? Why is output which to probably category K if category this is category K and it is category K that's going to be a one if it's not category K. It's going to be a zero. So ": [
            4358.4,
            4388.5,
            100
        ],
        "find my mouse on my screen. Okay. Voting is open. Oaks Okay, 68 of you voted. 72 I know there were 80 in here earlier just a few seconds ago. the other six of you haven't voted Okay, one more. Come on. You can do it make a choice. Not that are okay. I'm going to close it out going going going gone. What's the answer class? That's right. They're ": [
            3361.9,
            3416.8,
            76
        ],
        "for a long time. And it just screwed us. And that the learning really drives in your homework doesn't have that slope term. So it's funny. But that's what happened. Sometimes though. We told her students. Just leave out the slope learns better. We didn't we didn't know why though. So what's going on here is that mean squared error is the wrong objective function for logistic regression. The right ": [
            1653.5,
            1694.8,
            32
        ],
        "forgot the no. I can leave after learning right you don't need that. Okay. So now since these are all going to be the same basically for every n I'm just going to do one exact one example and get rid of having take all those Having all that clunky sigma's and stuff like that. Okay. example Okay, that is. One pattern I call it so pattern is an input ": [
            937.1,
            986.6,
            17
        ],
        "get this something. And the something is there for every data point so I can pull it out in front. So when I take the log of this I get this it's a song because I took the log until I get that part. Okay, and what is that? That looks a lot like some squared error scaled by this number, which doesn't have anything to do with it mean. ": [
            3854.2,
            3880.0,
            88
        ],
        "gives you linear regression where your home network is this and you have multiple in your DM put supposed to bias in D + 1 weights. And so you always get a line and you can use that with data and the Delta World to fit some data. And if you have a bunch of data and you use gradient descent on linear with with this formula you get that ": [
            1908.5,
            1947.5,
            39
        ],
        "if you already know what giave is you're just a subtraction in the X way from the slope of TV? because the slope of the events just G of 8 * 1 - TV All right. Let's just look at that for a second. Are there any questions about this? So in the end, I dropped in minus and there's another month and you know, it's just some the end. ": [
            1437.6,
            1470.6,
            27
        ],
        "if you converge properly, so that's regression. It's not classification. Are we seeing perceptrons and they do classification, but they're either zero or one so they can just do two categories. But of course you could have multiple outputs of a perceptron to all the weights would just be independent of one another and each perceptron is trying to figure it's part of the category. So if you had multiple ": [
            1947.5,
            1984.3,
            40
        ],
        "is expected w i x x 0 + the derivative of WW1 X-14 The Shack to Wi... What's the derivative of? WD XD sorry, if you can't see that with respect to win, so when you're taking a derivative. A partial derivative used him all the other variables are constant. So that's if that's not I then that's a constant. So the all of these go to zero except when ": [
            1317.3,
            1361.9,
            25
        ],
        "is for those of you to put a minus sign instead of a plus sign. That's what happened. The Delta rule to change the weight is a result of Just figuring out how to go downhill and gradient descent means that. We don't set the derivative of the objective function said it's a zero and solved because we can't self because a bunch of non-linear equations. Okay, good next question ": [
            156.0,
            190.7,
            2
        ],
        "is me the star symbol. I'm meditating on an auto encoder network switch and see what that is someday. So this is based on Bishop chapter 6 section 6167 and 6969 as the derivative of the softmax. You might want to look at that. It's not it's some crucial steps there. They're kind of missing but helps, okay. That's what we're going to talk about. How does it lead to ": [
            2408.4,
            2446.0,
            53
        ],
        "is over here, but this cashing is near zero over here. So it's made me this data very unlikely making this likely which is wrong. Right? It's not this doesn't happen a lot. How about that one? I know about that one. Yeah, okay. So we're trying to find a model that makes the data likely and if we're trying to fit a gaussian to some data, we try and ": [
            2833.0,
            2864.0,
            64
        ],
        "is parameterize by Debbie. Pretty standard notation of a little semicolon and then the parameters. Okay, the negative log likelihood of this is this. Right here is negative log just in front of all of this. I haven't changed anything here. Nothing behind the curtain. And now I move the login. not login logout login I get this Okay, so if you take the log of e to something you ": [
            3814.1,
            3854.2,
            87
        ],
        "is y equal to G of a or G of areas does logistic activation function and a is the weighted sum of the input. Sometimes I'll slip and call that the net input and what's x 0 here. five five five One. Yeah, it's one w0s Tobias. Just checking. Okay? Okay. Why did I get this again? Okay, so we're going to reduce the objective function by going downhill and ": [
            709.4,
            757.0,
            12
        ],
        "is. And so the input on that line the x i affect how fast you learn that weight. And so if you go down to zero, you're not learning very much. Where is this one that goes down 2-1 in plus one. You can learn faster with this. Until when we have a hidden Lair, it's we're going to have soon. You're not ready yet? So we have some inputs ": [
            2164.1,
            2199.4,
            46
        ],
        "it could But remember base rule. They're probably of w given Diaz probably D given w x probably W over the probably the data. Hey, everybody. Remember Bayes rule. That's so much. So this represents a world in which D is true and then you're trying to find out what the most probable there this etcetera. Okay, so this is called the likelihood and Bayes rule. This is called the ": [
            2519.1,
            2559.9,
            56
        ],
        "it hits 0 then it's on linearly. So this is nice in the Deep Network because what we're going to find out if she's still need the slope for the hidden units, you can't get rid of it there. It's great having T. Minus y at the output but you're going to have a slope at the hidden units and you can't you have to have it. So nice thing ": [
            2067.3,
            2093.7,
            43
        ],
        "last is 0 right? Yeah, you can do a private post to us and show us your code and maybe somebody will find the answer. I don't see anything wrong with that 5 p.m. Yeah, I don't know I can find out when you're using the way to just trained to do the lost right now too sure of that. Okay, I would you know, I often like print things ": [
            4683.9,
            4724.5,
            107
        ],
        "lead to cross it? for logistic regression okay, so logistic regression should output the probability of category 1 so we want the network to produce the probability that the input is in category one. That is the output. should be the probability category 1 given X So that's again basically categorization. So How can so that that's like a coin flip, right? And okay, so what's the likelihood of the ": [
            4018.2,
            4068.4,
            92
        ],
        "like I did last time except slightly different. So this is minus the derivative of the sun over all the patterns. X the target for that pattern minus y for that pattern quantity squared. Okay, and that's Delta. That's the difference between what you did and what you should have done. oh, and I'm going to put mean squared error. So 1 / n And I needed to down here ": [
            841.5,
            884.6,
            15
        ],
        "like scores on a test. But in there on that works we have inputs and outputs and now the likelihood looks like this so it's the product. Of all the data points of the problem joint probability of the input and the output that's what we want to maximize. That's the likelihood again. We're assuming that all the EX's and the t's are independent of one another all the valley ": [
            3474.6,
            3509.3,
            78
        ],
        "like yours the excise then we have some units that are in between the input the output and so they're hidden because he can't see him can only see the input the output and we don't know exactly where they're heading there who put them there but here they are and this could be these guys could be relo and these guys softmax and this is just the input to ": [
            2199.4,
            2227.1,
            47
        ],
        "likelihood of the data. That's usually written with the script out. Is the product of the probability of each data point if they're independent data points than the likelihood of the data is the probability of each data point. Which where do the data points go their ears X that's the data? Here's xn here's xn so we're plugging this it we ended the gas in. and we take this ": [
            2721.8,
            2755.8,
            61
        ],
        "maximize and probably the data given the parameters. So you're trying to find the parameters that make the data most likely. It doesn't maximize the loss. We want to minimize the loss. Doesn't maximize the parameters. I don't know exactly what that would mean make the parameters really really big. Not sure. but anyway Where is this not working? Nothing. Nothing is working. Okay. That's weird. Okay. Where's my little? ": [
            3247.4,
            3300.1,
            74
        ],
        "maximize the likelihood. So to do that it turns out it's usually more convenient. Not to maximize the likelihood but minimize the negative of the log of the likelihood. and then log is a monotonic function so the the maximum of that is the maximum of not that right. So if you have x y z and you apply to the law to them and why is bigger than x ": [
            2864.0,
            2903.3,
            65
        ],
        "me so far. So what we're trying to do is first we have to have some probability distribution over the data. Last four linear or just plain regressing we assumed that the distribution was gassy and because we assumed there was some underlying deterministic function to which we had a gaussian noise here because it's a coin flip. We use a Bernoulli distribution for the likelihood. Okay. So what's the ": [
            4224.1,
            4263.0,
            97
        ],
        "mean of it and set Mew and are gassy in to the meeting. So that's an exercise for you. Okay. Okay. Okay, I found more clicker questions. Don't leave. There's a clicker question. Okay. I didn't know this had a clicker question is. Okay, so this is a check your understanding clicker question. okay, maximum likelihood estimation Does it maximize the parameters of a distribution? I think I turned off. ": [
            3076.4,
            3127.6,
            71
        ],
        "mean sometimes finishing the war all the time. When I put all the time. Tell me all the time or the place is always the same thing for every input. Did you remember the Z score your test set? Okay. Are you sure you're reading in the test set properly or the holdouts it properly? UC San Diego podcast ": [
            4760.4,
            4803.7,
            109
        ],
        "minus age of action pen to subtracting from both sides. So the likelihood of the data looks like this product of a gaussian. And now instead of HVAC Savannah in there. I put my neural-net cuz that's what I want to match. I want to match that underlying deterministic function in spite of the noise. Okay. And this notation here why of excavation semicolon W emphasizes that our model here ": [
            3773.9,
            3814.1,
            86
        ],
        "minus? Why is Delta here too, but that is the Delta rule. Okay any questions? at that answer Okay. right Okay Ready Set Go. Hopefully this one will be easy. It looks like it's easy. Okay, very few of your getting fooled by anything here. okay, 73 people Advanced 74 when wrong answer go change to right answer. So they're wondering answer. Okay going going going gone. Okay, and what's ": [
            434.8,
            508.8,
            6
        ],
        "model that is we want the parameters W. These are the weights that maximize the probability of the W given the data so, you know in our models in a single layer Network are even a multi-layer network. We've got parameters and destruction of the network kind of sets the structure of the model and this this is finding the best parameters that the data make data would generate if ": [
            2485.9,
            2519.1,
            55
        ],
        "my point. Okay this to the zero that to the zero, that's the one that 2-0. okay, just any questions Okay. So we can write the entire likelihood has the product of the probabilities of all the data points. So now we have this and this and this and this should start to look kind of familiar. So we take the negative log of that forget a song and some ": [
            4425.6,
            4459.5,
            102
        ],
        "negative log likelihood of that? Well negative log of this is remove the log inside at the sum. He had log of that to the T and you get Wow, there it is cross entropy. Okay, so to recap. When we're doing two category classification if we assume the targets are Bernoulli distribution and maximize the likelihood of data by minimizing the negative log likelihood. We find that we need ": [
            4263.0,
            4305.1,
            98
        ],
        "network regression thing etcetera. So if we make this assumption that when were trying to do regression the data has gassy and 0 Min gassy and noise. Then the probability of any one of these targets are given where it is. It has to be where it is because it's around this red line. Yes, actually so h of X event is we assume that's the the equation for the ": [
            3703.7,
            3738.0,
            84
        ],
        "of that. I have to use the chain rule again. And now I'm going to replace y with G of a So I've got my ass tribute of G of a with respect to Wi. And so it's t- why? dust and minus sign here privative G of a with respect to a x to a position Vector WI So I'm just using the chain rule. Again, so what's this? ": [
            1110.2,
            1153.3,
            21
        ],
        "of the trade, which is based on an older paper by young laocoon, but some of them are relevant and don't make sense. Yeah, actually I have nobody can solve my problem creasing instead of decreasing. Yeah increasing. I mean, yeah, it hurts my training to test it for two rows gradually decreasing 0 and then I asked her if she calls me to figure it out. Maybe you're learning ": [
            4504.5,
            4568.9,
            104
        ],
        "on it really unless somebody wants me to. Okay, why is this called a softmax? You know, whoever has the biggest a is going to win right there going to be the biggest output. So why so the max winds but why we use this at all? Well, the reason again is that it it's positive that's always a positive number. And when you send them over all the patterns. ": [
            1760.2,
            1798.1,
            35
        ],
        "one is the one you used cross entropy. Which leads to the Delta rule? And in the next lecture, maybe this one and I'm pretty sure this one cuz we got 40 got a long time left 15 minutes. Yeah, we'll get into it. Okay, so any questions, so I just arrived a bad learning real for you and that's when we use for a long time. No questions, no ": [
            1694.8,
            1725.6,
            33
        ],
        "or I have to start it again. The answer is still not easy. Okay. going going going God Okay, you your neighbor has a 49% chance of having a great turn to your neighbor and discuss it will try again. You got to find a neighbor if you don't have a neighbor. doing Okay, it seem to have settled down. Got your answer in your pocket ready. Carol better Okay, ": [
            190.7,
            333.4,
            3
        ],
        "other side of the line. They're off. And then we seen logistic regression. and softmax And all of these can be trained by the Delta rule. It's kind of spooky actually. There are other activation functions though. They were going to see soon one is called rectified linear units are rellos to his friends. And this is kind of a combination of linear and and and perceptron it's off until ": [
            2027.9,
            2067.3,
            42
        ],
        "out to see make sure they're what I think they are. You know, if you have tried that I see what's going on. Crab Shack I need a way to get updated and has lost all the secrecy. Can you just look at what the output of the network is for your test examples? It is like just something easy reminder right but does it make sense at all? I ": [
            4724.5,
            4760.4,
            108
        ],
        "output pattern. Okay, so I'm going to want to do the derivative of 10 - y n quantity squared. Why over two times in front of your ear? weather check your WI and so that's equal to 2/2. X t r n o I can drop the ends. Sorry. And drop to you and see a girl. Can you smoke a T minus y x the derivative of C minus ": [
            986.6,
            1028.5,
            18
        ],
        "outside and there's a big n on it and now we're chronic thing over every datapoint E2 that okay. That's so what happens? So maximum likelihood says we should pick you and sigma such that the likely it is maximized. and Until we want to maximize this which means we want to maximize this we want to find the Mew and sigma that make this as big as possible. Okay, ": [
            2755.8,
            2798.3,
            62
        ],
        "people. Hopefully this won't be too fast. So Rivera's you just joining us. We already did the clickers. Sorry. Okay, so the schema is w i s w i minus some learning rate from Zagreb native of MSC. With the respective wink, and we're going to try to do logistic regression this way. Okay, so I have to figure out what this is. And it's going to look a lot ": [
            796.2,
            841.5,
            14
        ],
        "prior. You know, what's the probability of that happening if I don't know anything else and this is a normalizing constant? So we don't have to think about that. And we don't have any reason to assume some W's are better than others. We can make some assumptions about the double using that leads to Bayesian backpropagation. But you know like the W's might be gassy and describe distributed. Right ": [
            2559.9,
            2599.0,
            57
        ],
        "questions. Okay. So for logistic regression, there's no clothes formula for the form formula for the weight. So we have to use gradient descent as you guys have been doing in your homework. And I said all this cross entropy lines leads to A good rule. Okay. So this is just the softmax you all know what this off Max's by now, so I'm not going to spend any time ": [
            1725.6,
            1760.2,
            34
        ],
        "rate is too high and weight. Airplanes lyrics smaller number increase laptop, but still interested. Okay, so nobody can find out which one so, where is this where you're changing the weights? Yeah, so why it's got the label label - 1/4 Mi South product key for this and why for that? Yeah, okay. Tell me how I am just seems cracked. I'm sure this is hard for me to ": [
            4568.9,
            4617.6,
            105
        ],
        "read cuz these are all different things in my operation. So you could Logistics. Do something again? To hear you're doing what it is at a Timber at a 1/2 as a woman x o what's encode Logistics base is in a then it is what it is or why they need to get their own. Okay. Okay. Okay 13 live doppler. Do you have you tried resetting the validation ": [
            4617.6,
            4683.9,
            106
        ],
        "red line sorry, that was a little I want a little too far ahead. So this is the Red Line This is the deterministic function. And this is the probability of sum Target given there. So there's some deterministic function with some zero mean additive Galaxy noise. So a Target is HVAC Savannah at that point plus Epsilon for Epsilon is the gaussian noise. well That means Epsilon equals T ": [
            3738.0,
            3773.9,
            85
        ],
        "separable at the output. So there are theoretical ways to motivate perceptrons and these other things and we're going to talk about that more next time which is really going to be this time. So let's go to this time. Okay, so I haven't changed this one yet. But I told you that you know, I had people that. It didn't like this. It made them hurt their eyes or ": [
            2325.1,
            2364.2,
            51
        ],
        "so we should choose the parameters that maximize the likelihood of the data. How do we do that? So here's an illustration. This is the distribution of scores from CSC 150 and spring of 2016. I don't know which way is high and which way is low, but here's the data. It's looks kind of gaussian ish. Does this go see and make the data likely? Show all the date ": [
            2798.3,
            2833.0,
            63
        ],
        "some squared error for aggression has it lead to cross tent for logistic regression and how does it lead to cross entropy for multinomial? Regression, okay. So what is maximum likelihood? That's where we're going to start. So the main idea is what we really want to know is given the data which parameters of w of our model are most likely. What are the most probable parameters of our ": [
            2446.0,
            2485.9,
            54
        ],
        "straightforward logs blah blah blah, okay. No questions, okay. So since in a neural network remodeling the mapping from x2t. When there's no parameters in here of the model, this is what we're trying to model the probably the output given the input. Right. That's what logistic regression does. Its Woodstock Max regression does. So when we're minimizing this for their structure are parameters we can just drop this term. ": [
            3547.0,
            3587.0,
            80
        ],
        "summer big some are small that summer zero. Okay. So we're going to assume without any data that all W's are equally likely so this prior is a constant. That is it's the same for everything. That's the uniform distribution. Okay. So what happens then? So if this is normalizing constant, and this is constant, then maximizing this means we're maximizing this over W, which means we're maximizing this cuz ": [
            2599.0,
            2639.8,
            58
        ],
        "that in some sense because we're going to start with me and squirt are take the derivative with respect to the weights plug the result into the formula for gradient descent and get a bad learning room. And in the next the next lecture we might get to some of it today. We'll see why that is, okay. So the just to remember our notation the output of the network ": [
            674.3,
            709.4,
            11
        ],
        "that's constant and that's constant. So this is a common thing that's done in machine learning is Hugh. Try and make this High by making this high. And so that's that's where the name comes from. This is the likelihood and we're trying to maximize the likelihood. Bayesian hate this they have priors and so we're not being real bajans here. So we want so what this says is I ": [
            2639.8,
            2678.0,
            59
        ],
        "the answer be so one person said this is the mean squared error objective function. It's not the logistic function. This is the logistic function otherwise known as the sigmoid or the the squashing function. And that's why it's called the sigmoid again cuz it looks like this. Where is his five at zero and only one asymptotic Lee + - + 0 asymptotic Lee? Okay, any coach don't suppose ": [
            508.8,
            556.8,
            7
        ],
        "the slope is the gradient is going to be very small. Okay. What if Juve is really close to 1? Same problem, right? So we're out here or out here and if we're so. If we're very confident in our answering wrong, it's really hard to change it. But if we're really confident in her answer and were wrong and we used the standard Delta roll that we got from ": [
            1583.0,
            1619.1,
            30
        ],
        "the things we're trying to fit and we're trying to find Sigma. and so if you Try and figure out what Miu is he set this equal to zero and take the derivative of this and Saturday equal to zero and solve. That ends up setting Mewtwo the empirical meaning of this data. That's why when we're trying to fit a Galaxy and we use the data we compute the ": [
            3041.9,
            3076.4,
            70
        ],
        "the weights are the parameters we have and plugging the result into the format formula for gradient descent. And then it pops the Delta roll. Okay. What happens if we try to use mean squared error for logistic regression? So today I'm going to start with me and you guys are all done the derivative for logistic regression. I hope softmax is harder and I don't blame you. If you ": [
            609.1,
            642.1,
            9
        ],
        "their brains or something. I'm not sure what Okay, the white on Blue instead of black on what kind of just didn't work for my research group believe more Research Unit Guru. We do unbelievable research which is why we can't get published. That's not true. We do get published. I had somebody in a review Wednesday. He can't even get his stuff published man. It's a joke. And this ": [
            2364.2,
            2408.4,
            52
        ],
        "there any questions about that? Okay. that's it for clicker questions today has been Okay. Okay. So some easy ones murder ones. Okay, so last time MC. I guess it's a great presentation turn on some more lights. So last time I derive the Delta rule for linear regression by coming up with me and screw starting with mean squared error taking the derivative with respect to the weights and ": [
            556.8,
            609.1,
            8
        ],
        "there's a little something going on down here. Another line at some slow. It's okay tan H is another one that goes from -1 to 1 and this turns out to be useful because it's not 0 or 1. So if I'm in a deep Network where I am sending activation up to the next guy something very still the Delta Roy just have to figure out what the Delta ": [
            2130.1,
            2164.1,
            45
        ],
        "these are always going to be either linear. If you're trying to do regression or they and that and because of this it's no longer linear regression. You can fit curves and things. But out here if you're doing classification, it's either the logistic or softmax. and disappointed people don't often think about a lot of times what you're trying to do here is something it's not linearly separable. So ": [
            2258.7,
            2296.9,
            49
        ],
        "this is the schema again for 4 gradient descent doesn't matter what this is or how this figures into the model doesn't have to be neural Nets could be something else you try and go downhill with in the error with respected that parameter. Okay, so I have a lot of slides that do all this but I think it turns out that you know, that's too fast for most ": [
            757.0,
            796.2,
            13
        ],
        "to minimize cross entropy error. What's he doesn't play? so I'm in 8 minutes. How does this lead to cross and for multinomial regression? Okay. Now we have more than 2 outputs. Let's assume they're see outputs one for each category now he want. I lit the probably is that the case output is the probability that that input is in category k? And we want T soup and k ": [
            4305.1,
            4358.4,
            99
        ],
        "to the 1 power which is just y ven. So that's what our network is saying is the output 1 - y ven * 1 - but wait even is one this becomes a zero. And so this goes away cuz it's this to the zeroth power. So we just end up with Y ven. What if it's in Category 2? What's the probability of it being in category too? ": [
            4154.2,
            4184.2,
            95
        ],
        "to. Okay. So that's the derivative of SEC CWI. Now the derivative of the sun is the sum of the derivatives again. So that's just - 1 / end and the Sun. Ozone goes from 1 to figgin of 1/2. The derivative T of N - y van. quantity squared with respect to Wi Okay, that's fine. Everybody happy with that. I hope ya did I do it, right? I ": [
            884.6,
            937.1,
            16
        ],
        "unit E2 this? So when you take the natural log of e to something you got that something. Doodoo, doodoo doodoo doodoo, what does that Look Alot Like that looks a lot like squared error. Got a minus here. It's got a minus here. And some things out front something down here. Usually if we're trying to so what we're trying to do is find the mean. That's one of ": [
            3006.3,
            3041.9,
            69
        ],
        "want W's that make the data that we get the info in our case the input-output function as likely as possible. It's not the input. It makes the data likely as possible. So how we model that distribution of data is the key thing. So here's a Galaxy and distribution. Assuming the data points are independently identically distributed and we're trying to fit this cash into some data. Then the ": [
            2678.0,
            2721.8,
            60
        ],
        "we maximize the likelihood of the data by minimizing the negative log likelihood we get some screws are. Okay, pretty cool. Huh? Any questions? Yeah. When was this discovered? probably by gas or something like that so old this is if you're doing regression, that's not logistic regression. This is just fitting data not trying to categorize stay there. We'll get to that. Don't worry coming. So how does this ": [
            3966.2,
            4018.2,
            91
        ],
        "we went from. 49% getting it right to 57% 58% That's the that's the power of peer instruction make a k going going going gone. Okay. What's the answer class? the answer is d four of you said see and and this will cause your air to explode. Time do not do that. A third of you or not said be now be is an answer to a different question. ": [
            333.4,
            392.5,
            4
        ],
        "what the network is trying to do with these layers you can have lots of layers here not just one is get them to the point. So that just before the output. You're making everything linearly separable again. Cuz that's all you can do with these guys. So you're trying to find learn some features in the service of the task that will make it linear make your problem linearly ": [
            2296.9,
            2325.1,
            50
        ],
        "y with respect to Wi. So that's just using the chain rule of calculus the two comes down and that's why I've got a 1/2 in front cuz now I've got t- why? Times in somewhere back there. I dropped a minus sign to So the derivative of the sum is the sum of the derivatives against oh, yeah - the derivative of y. Spectra WI and that is 0 ": [
            1028.5,
            1063.6,
            19
        ],
        "you're taking a partial derivative. What are the variables here he's not at sexist in the training said it's a constant double use a variable and the derivative of the sun is the sum of the derivative. So this is really jaeckels 1/2 D of the derivative of w 0 x 0 with respect to Wi her. Sorry. Make this a little more explicit. This is the derivative of w0 ": [
            1276.9,
            1317.3,
            24
        ]
    },
    "File Name": "Deep Learning - C00 - Cottrell, Garrison W - Fall 2018-lecture_4.flac",
    "Full Transcript": "Listen to a podcast.  Okay.  I'm going to start right off with a few clicker question.  Sarah get your clickers out.  and should be a a  I haven't been able to find my  Professor clicker so I don't can't change the channel. So I hope no one next door is using clickers on a okay. You ready?  Set Go  Oh, I have to say sorry. I haven't started it yet.  Okay.  There we go. Okay.  Okay.  The answer is not he.  are you want to put it there, but  Okay, is everybody voted for their only 65 of you?  Really? Okay 66.  Okay.  I'm going to close it out.  going going  Going Sean. Okay, what's the answer? Thank you. 94% of you got that for four people.  I didn't get it. Why is he answer not be?  Yeah, this this is for those of you to put a minus sign instead of a plus sign. That's what happened. The Delta rule to change the weight is a result of  Just figuring out how to go downhill and gradient descent means that.  We don't set the derivative of the objective function said it's a zero and solved because we can't self because a bunch of non-linear equations.  Okay, good next question or I have to start it again.  The answer is still not easy.  Okay.  going going going  God  Okay, you your neighbor has a 49% chance of having a great turn to your neighbor and discuss it will try again.  You got to find a neighbor if you don't have a neighbor.  doing  Okay, it seem to have settled down. Got your answer in your pocket ready.  Carol  better  Okay, we went from.  49% getting it right to 57% 58%  That's the that's the power of peer instruction make a  k  going  going going gone. Okay. What's the answer class?  the answer is d  four of you said see and and this will cause your air to explode.  Time do not do that.  A third of you or not said be now be is an answer to a different question.  This is the this is the schema for gradient descent. If you want to change your weights, you should change them in the direction of that will.  Decrease the objective function. So this part says uphill in the objective function When You Subtract it you go downhill.  Okay.  This is the Delta. So I tried to fill you with this because this is delta T. Minus. Why is delta T minus? Why is Delta here too, but that is the Delta rule.  Okay any questions?  at that answer  Okay.  right  Okay Ready Set Go.  Hopefully this one will be easy.  It looks like it's easy.  Okay, very few of your getting fooled by anything here.  okay, 73 people Advanced 74  when wrong answer go change to right answer.  So they're wondering answer.  Okay going going going gone.  Okay, and what's the answer be so one person said this is the mean squared error objective function. It's not the logistic function. This is the logistic function otherwise known as the sigmoid or the the squashing function.  And that's why it's called the sigmoid again cuz it looks like this.  Where is his five at zero and only one asymptotic Lee + - + 0 asymptotic Lee? Okay, any coach don't suppose there any questions about that? Okay.  that's it for clicker questions today has been  Okay.  Okay.  So some easy ones murder ones. Okay, so last time  MC. I guess it's a great presentation turn on some more lights. So last time I derive the Delta rule for linear regression by coming up with me and screw starting with mean squared error taking the derivative with respect to the weights and the weights are the parameters we have and plugging the result into the format formula for gradient descent.  And then it pops the Delta roll.  Okay.  What happens if we try to use mean squared error for logistic regression?  So today I'm going to start with me and you guys are all done the derivative for logistic regression. I hope softmax is harder and I don't blame you. If you don't get it still turn in your programming spent minutes and your homework, but that is a little tricky of a derivative. So don't feel bad. If you can't figure it out. Okay? Okay, so we're going to start today. We're going to start with me and squirt or not. So what we did in the homework was start with cross entropy.  And you're going to see why we did that in some sense because we're going to start with me and squirt are take the derivative with respect to the weights plug the result into the formula for gradient descent and get a bad learning room. And in the next the next lecture we might get to some of it today. We'll see why that is, okay.  So the just to remember our notation the output of the network is y equal to G of a or G of areas does logistic activation function and a is the weighted sum of the input. Sometimes I'll slip and call that the net input and what's x 0 here.  five five five  One. Yeah, it's one w0s Tobias. Just checking. Okay?  Okay.  Why did I get this again? Okay, so we're going to reduce the objective function by going downhill and this is the schema again for  4 gradient descent doesn't matter what this is or how this figures into the model doesn't have to be neural Nets could be something else you try and go downhill with in the error with respected that parameter.  Okay, so I have a lot of slides that do all this but I think it turns out that you know, that's too fast for most people. Hopefully this won't be too fast. So  Rivera's you just joining us. We already did the clickers. Sorry.  Okay, so the schema is w i s w i  minus some learning rate from Zagreb native of MSC.  With the respective wink, and we're going to try to do logistic regression this way. Okay, so I have to figure out what this is.  And it's going to look a lot like I did last time except slightly different. So this is minus the derivative of the sun over all the patterns.  X  the target for that pattern minus y for that pattern quantity squared.  Okay, and that's Delta. That's the difference between what you did and what you should have done.  oh, and I'm going to put  mean squared error. So 1 / n  And I needed to down here to.  Okay.  So that's the derivative of SEC CWI. Now the derivative of the sun is the sum of the derivatives again. So that's just - 1 / end and the Sun.  Ozone goes from 1 to figgin of 1/2.  The derivative T of N - y van.  quantity squared with respect to Wi  Okay, that's fine. Everybody happy with that. I hope ya did I do it, right?  I forgot the no.  I can leave after learning right you don't need that. Okay. So now since these are all going to be the same basically for every n I'm just going to do one exact one example and get rid of having take all those  Having all that clunky sigma's and stuff like that. Okay.  example  Okay, that is.  One pattern I call it so pattern is an input output pattern.  Okay, so I'm going to want to do the derivative of 10 - y n quantity squared.  Why over two times in front of your ear?  weather check your WI  and so that's equal to 2/2.  X t r n o I can drop the ends. Sorry.  And drop to you and see a girl. Can you smoke a T minus y x the derivative of C minus y with respect to Wi.  So that's just using the chain rule of calculus the two comes down and that's why I've got a 1/2 in front cuz now I've got t- why?  Times in somewhere back there. I dropped a minus sign to  So the derivative of the sum is the sum of the derivatives against oh, yeah - the derivative of y.  Spectra WI  and that is 0 cuz T is a constant.  and so  You got th - y.  X the derivative of y -2 driven is a y or w i but why she of a is a so I can just do this, right?  No, I can't because this is logistic regression. So y equals g of a  How much is 1/1 + 80 to the minus a okay, so I can't just get rid of that. I have to use the chain rule again. And now I'm going to replace y with G of a  So I've got my ass tribute of G of a with respect to Wi.  And so it's t- why?  dust and minus sign here privative G of a with respect to a x to a position Vector WI  So I'm just using the chain rule.  Again, so what's this?  What's this one?  That's just any anybody somebody what?  I just you can leave it in terms of G.  Where does it?  Yeah.  That's after we figure out what this is first, which is high.  Okay, what's the derivative of f of x with respect to X?  F Prime of X the derivative of x at that point right? So  so we get  so we get  3 - y  x - IG Prime  Of a the slope of g at that point x the derivative of a with respect to Wi.  Okay, and what is AA again is the weighted sum of the inputs?  so this is the derivative of the Sun from I or sorry jay equals 1/2 D of w i x i s r e j j j j was expected ww-ii  Ask Sai why because when you're taking a partial derivative.  What are the variables here he's not at sexist in the training said it's a constant double use a variable and the derivative of the sun is the sum of the derivative. So this is really jaeckels 1/2 D of the derivative of w 0 x 0 with respect to Wi her. Sorry.  Make this a little more explicit.  This is the derivative of w0 is expected w i x x 0 + the derivative of WW1 X-14 The Shack to Wi...  What's the derivative of?  WD XD sorry, if you can't see that with respect to win, so when you're taking a derivative.  A partial derivative used him all the other variables are constant.  So that's if that's not I then that's a constant. So the all of these go to zero except when J equal sign.  and so  that's t- why?  x - G Prime of a  time's the derivative of w i x is expected wi, which is just X.  so  Okay, so that okay. So there it is. That's what you get for learning rule.  when you use mean squared error with the logistic function  Okay, so no no, no, no.  Okay, so  all right Sue.  Okay, so fun fact.  That if you already know what giave is you're just a subtraction in the X way from the slope of TV?  because the slope of the events just G of 8 * 1 - TV  All right. Let's just look at that for a second. Are there any questions about this?  So in the end, I dropped in minus and there's another month and you know, it's just some the end.  Okay, so  here's the squashing function.  And the slope is this * 1 - this that means it's going to be like that.  It's sort of like a normal.  Hey, so that's tea Prime.  Arvak's enough T-Rex  money maker  Okay.  So that's that's cool. We were happy about that. We didn't have to do much to calculate the slope.  But what's wrong with this? It looks a lot like the Delta rule.  so any ideas  There's nothing wrong with the math math is all good.  For what could be a problem with this.  What if  Yeah, what if giave is already close to zero? So the output of the network is almost zero what happens to the learning?  for that pattern  You know, it's always G of a of N and that's going to be very close to zero in the slope is the gradient is going to be very small.  Okay. What if Juve is really close to 1?  Same problem, right? So we're out here or out here and if we're so.  If we're very confident in our answering wrong, it's really hard to change it.  But if we're really confident in her answer and were wrong and we used the standard Delta roll that we got from Cross entropy. No problem, you know, this isn't here if we're really confident and wrong. We're going to get a big slope there.  You got a big error signal?  All right.  So not knowing any better four years. This is what we did. This is what's in chapter 8 of the Old Testament that PDP book volume 1 and if it does work, I mean this did we use this for a long time.  And it just screwed us.  And that the learning really drives in your homework doesn't have that slope term. So it's funny. But that's what happened. Sometimes though. We told her students. Just leave out the slope learns better.  We didn't we didn't know why though.  So what's going on here is that mean squared error is the wrong objective function for logistic regression. The right one is the one you used cross entropy.  Which leads to the Delta rule?  And in the next lecture, maybe this one and I'm pretty sure this one cuz we got 40 got a long time left 15 minutes. Yeah, we'll get into it.  Okay, so any questions, so I just arrived a bad learning real for you and that's when we use for a long time.  No questions, no questions. Okay. So for logistic regression, there's no clothes formula for the form formula for the weight. So we have to use gradient descent as you guys have been doing in your homework.  And I said all this cross entropy lines leads to A good rule.  Okay. So this is just the softmax you all know what this off Max's by now, so I'm not going to spend any time on it really unless somebody wants me to.  Okay, why is this called a softmax?  You know, whoever has the biggest a is going to win right there going to be the biggest output. So why so the max winds but why we use this at all? Well, the reason again is that it it's positive that's always a positive number.  And when you send them over all the patterns.  We get this on the top and this on the bottom, so it sums to one.  So it gives you a probability distribution over the categories.  And we'll be seeing the softmax again fairly often. So that's sometimes called a softmax distribution.  So if anybody goes up somebody else goes down. It's like a winner-take-all network. So that's what we used to call it in the old days. She had a network with multiple answers and they were all inhibiting one another the guy with the most input usually would win. So that's how we make decisions in their own that send you make a decision here by picking the maximum output right say I had a student in my office hour today saying he was getting 100% on the 6-way one. Anybody getting that. Okay. That's a bug whoever-you-are. He's got a bug somewhere.  Okay, if you're still here, okay?  So we can take it to logistic is that the softmax is actually a generalization of logistic. If you plug into the soft Max just two categories. You can turn that into a soft into a logistic.  So it's a generalization to more than two categories.  So we've seen four kinds of neural networks so far linear Network, which gives you linear regression where your home network is this and you have multiple in your DM put supposed to bias in D + 1 weights.  And so you always get a line and you can use that with data and the Delta World to fit some data. And if you have a bunch of data and you use gradient descent on linear with with this formula you get that if you converge properly, so that's regression. It's not classification.  Are we seeing perceptrons and they do classification, but they're either zero or one so they can just do two categories.  But of course you could have multiple outputs of a perceptron to all the weights would just be independent of one another and each perceptron is trying to figure it's part of the category. So if you had multiple  categories  You know.  one perceptron might  shut off all the pictures of Gary and one might cut off all the pictures of Bob.  One might cut off all the pictures of Ted.  So they're all fighting it out.  Okay.  And perceptrons again can be considered a linear discriminant. All they do is put a line down in the input space and on one side of the line. They're on the other side of the line. They're off.  And then we seen logistic regression.  and softmax  And all of these can be trained by the Delta rule. It's kind of spooky actually.  There are other activation functions though. They were going to see soon one is called rectified linear units are rellos to his friends. And this is kind of a combination of linear and and and perceptron it's off until it hits 0 then it's on linearly.  So this is nice in the Deep Network because what we're going to find out if she's still need the slope for the hidden units, you can't get rid of it there. It's great having T. Minus y at the output but you're going to have a slope at the hidden units and you can't you have to have it. So nice thing about this is if you've got it as a hidden unit is that the slopes really easy one year and zero here. There's like nothing to do basically, right?  The weird thing about it in terms of like a neural network is real neurons. Don't just keep going as high as they can go. They have a maximum firing rate.  So that's one. There's also one called leaky relu where there's a little something going on down here. Another line at some slow.  It's okay tan H is another one that goes from -1 to 1 and this turns out to be useful because it's not 0 or 1. So if I'm in a deep Network where I am sending activation up to the next guy something very still the Delta Roy just have to figure out what the Delta is. And so the input on that line the x i affect how fast you learn that weight. And so if you go down to zero, you're not learning very much. Where is this one that goes down 2-1 in plus one. You can learn faster with this.  Until when we have a hidden Lair, it's we're going to have soon.  You're not ready yet?  So we have some inputs like yours the excise then we have some units that are in between the input the output and so they're hidden because he can't see him can only see the input the output and we don't know exactly where they're heading there who put them there but here they are and this could be these guys could be relo and these guys softmax and this is just the input to a lot of mistakes people made early on your friend of mine still does this is a fine activation function to the input switch, you know, the logistic it's just kind of weird, but you don't do that. So is it just the the Ryan puts that and then this is where we learn so  These could be rather they could be Logistics. They could be 10 H. And typically these are always going to be either linear. If you're trying to do regression or they and that and because of this it's no longer linear regression. You can fit curves and things.  But out here if you're doing classification, it's either the logistic or softmax.  and disappointed people don't often think about  a lot of times what you're trying to do here is something it's not linearly separable.  So what the network is trying to do with these layers you can have lots of layers here not just one is get them to the point. So that just before the output.  You're making everything linearly separable again.  Cuz that's all you can do with these guys.  So you're trying to find learn some features in the service of the task that will make it linear make your problem linearly separable at the output.  So there are theoretical ways to motivate perceptrons and these other things and we're going to talk about that more next time which is really going to be this time.  So let's go to this time.  Okay, so I haven't changed this one yet.  But I told you that you know, I had people that.  It didn't like this. It made them hurt their eyes or their brains or something. I'm not sure what  Okay, the white on Blue instead of black on what kind of just didn't work for my research group believe more Research Unit Guru. We do unbelievable research which is why we can't get published. That's not true. We do get published. I had somebody in a review Wednesday. He can't even get his stuff published man. It's a joke.  And this is me the star symbol. I'm meditating on an auto encoder network switch and see what that is someday. So this is based on Bishop chapter 6 section 6167 and 6969 as the derivative of the softmax. You might want to look at that. It's not it's some crucial steps there. They're kind of missing but helps, okay.  That's what we're going to talk about. How does it lead to some squared error for aggression has it lead to cross tent for logistic regression and how does it lead to cross entropy for multinomial?  Regression, okay.  So what is maximum likelihood?  That's where we're going to start. So the main idea is what we really want to know is given the data which parameters of w of our model are most likely.  What are the most probable parameters of our model that is we want the parameters W. These are the weights that maximize the probability of the W given the data so, you know in our models in a single layer Network are even a multi-layer network. We've got parameters and destruction of the network kind of sets the structure of the model and this this is finding the best parameters that the data make data would generate if it could  But remember base rule.  They're probably of w given Diaz probably D given w x probably W over the probably the data.  Hey, everybody. Remember Bayes rule.  That's so much. So this represents a world in which D is true and then you're trying to find out what the most probable there this etcetera.  Okay, so this is called the likelihood and Bayes rule. This is called the prior. You know, what's the probability of that happening if I don't know anything else and this is a normalizing constant?  So we don't have to think about that.  And we don't have any reason to assume some W's are better than others. We can make some assumptions about the double using that leads to Bayesian backpropagation. But you know like the W's might be gassy and describe distributed. Right summer big some are small that summer zero. Okay. So we're going to assume without any data that all W's are equally likely so this prior is a constant. That is it's the same for everything. That's the uniform distribution. Okay.  So what happens then?  So if this is normalizing constant, and this is constant, then maximizing this means we're maximizing this over W, which means we're maximizing this cuz that's constant and that's constant.  So this is a common thing that's done in machine learning is Hugh.  Try and make this High by making this high.  And so that's that's where the name comes from. This is the likelihood and we're trying to maximize the likelihood.  Bayesian hate this they have priors and so we're not being real bajans here. So we want so what this says is I want W's that make the data that we get the info in our case the input-output function as likely as possible.  It's not the input. It makes the data likely as possible. So how we model that distribution of data is the key thing.  So here's a Galaxy and distribution.  Assuming the data points are independently identically distributed and we're trying to fit this cash into some data.  Then the likelihood of the data.  That's usually written with the script out.  Is the product of the probability of each data point if they're independent data points than the likelihood of the data is the probability of each data point.  Which where do the data points go their ears X that's the data?  Here's xn here's xn so we're plugging this it we ended the gas in.  and we take this outside and there's a big n on it and now we're chronic thing over every datapoint E2 that  okay.  That's so what happens?  So maximum likelihood says we should pick you and sigma such that the likely it is maximized.  and  Until we want to maximize this which means we want to maximize this we want to find the Mew and sigma that make this as big as possible.  Okay, so we should choose the parameters that maximize the likelihood of the data.  How do we do that?  So here's an illustration. This is the distribution of scores from CSC 150 and spring of 2016. I don't know which way is high and which way is low, but here's the data. It's looks kind of gaussian ish. Does this go see and make the data likely?  Show all the date is over here, but this cashing is near zero over here. So it's made me this data very unlikely making this likely which is wrong. Right? It's not this doesn't happen a lot.  How about that one?  I know about that one. Yeah, okay. So we're trying to find a model that makes the data likely and if we're trying to fit a gaussian to some data, we try and maximize the likelihood.  So to do that it turns out it's usually more convenient.  Not to maximize the likelihood but minimize the negative of the log of the likelihood.  and then log is a  monotonic function so the  the maximum of that is the maximum of  not that right. So if you have x y z and you apply to the law to them and why is bigger than x and z is bigger than y then the log of those are in the same order, right? It's a monotonic function doesn't change. What where the peak is. So often what we do is we stick a log in front of that.  We are.  The max of this then is the minimum of this with a negative sign in front.  All right. That's all I did there.  And there should be some parentheses around this whole thing and not subtracting this from the argument. Okay, so the cool thing that happens when you take the log  Is it turns into a sum and then you have the log of the probability of each point?  Okay, any questions so far? So typically we call this the error.  Right try to maximize the likelihood but instead we minimize the negative log likelihood and we usually just call that the air.  So that's the likelihood that's the log likelihood. It's the negative log likelihood to come here. Okay. I already said that.  So these are all equivalent finding the maximum of the log finding the minimum of the negative log fighting the minimum this thing.  Find the minimum of this thing.  I guess what we had a Galaxy in here.  What's a gas unit E2 this?  So when you take the natural log of e to something you got that something.  Doodoo, doodoo doodoo doodoo, what does that Look Alot Like that looks a lot like squared error.  Got a minus here. It's got a minus here.  And some things out front something down here. Usually if we're trying to so what we're trying to do is find the mean.  That's one of the things we're trying to fit and we're trying to find Sigma.  and so if you  Try and figure out what Miu is he set this equal to zero and take the derivative of this and Saturday equal to zero and solve.  That ends up setting Mewtwo the empirical meaning of this data.  That's why when we're trying to fit a Galaxy and we use the data we compute the mean of it and set Mew and are gassy in to the meeting.  So that's an exercise for you.  Okay.  Okay.  Okay, I found more clicker questions. Don't leave. There's a clicker question.  Okay.  I didn't know this had a clicker question is.  Okay, so this is a check your understanding clicker question.  okay, maximum likelihood estimation  Does it maximize the parameters of a distribution?  I think I turned off.  I must have turned off and I see.  Start new session, okay.  Start new session.  Okay.  Where'd my little thing go? There it is. Okay.  Mariah Carey  Okay liquor is live.  Hey, is it 59 of you 60 of answered?  Okay.  What's your answer?  76 of you've answered  and oh, I haven't looked at the distribution of answers.  Okay, good. Okay, it's more than 80% of you. That means I don't have to go to talking to your neighbor.  That's that's what I learn from Christine Alvarado. Okay.  Okay.  Now, it's 70.  Okay going to close it out going going going gone evokes. What's the answer class?  See, okay good. So a few people voted for d  And at least four people voted for a so maximum likelihood maximizes the probability of the data.  Given, you know you trying to maximize and probably the data given the parameters. So you're trying to find the parameters that make the data most likely.  It doesn't maximize the loss. We want to minimize the loss.  Doesn't maximize the parameters.  I don't know exactly what that would mean make the parameters really really big. Not sure.  but anyway  Where is this not working?  Nothing. Nothing is working.  Okay.  That's weird.  Okay.  Where's my little?  Okay.  All right.  question 2  the following is an expression. No, I'm not going to  oh, how do I find my friend there, please?  Where's my mouse? This isn't showing up on my screen is my mouse.  Okay.  So this is the results of the maximum likelihood estimation for a 1D. Gaussian.  So it's an expression for the results of mle.  Okay.  Oh, I haven't started.  Sorry.  I have to find my mouse on my screen.  Okay.  Voting is open.  Oaks  Okay, 68 of you voted.  72 I know there were 80 in here earlier just a few seconds ago.  the other six of you  haven't voted  Okay, one more. Come on. You can do it make a choice.  Not that are okay. I'm going to close it out going going going gone. What's the answer class? That's right. They're all the same.  I'm trying to maximize the log likelihood. We're trying to minimize the negative log likelihood.  We're trying to maximize the likelihood.  D&C  Let's see if it's this works down doesn't work if this works.  Well, it stopped again.  Okay.  All right, so that's it for clickers for the moment. So now you can go.  Okay.  Okay, so  Or finding a Galaxy and we assumed one dimensional data like scores on a test. But in there on that works we have inputs and outputs and now the likelihood looks like this so it's the product.  Of all the data points of the problem joint probability of the input and the output that's what we want to maximize. That's the likelihood again. We're assuming that all the EX's and the t's are independent of one another all the valley all the data points in the in the training set are independent so we can product them together.  But this factors if you remember your probability in the probability of T given x times the probability of x.  And taking the negative log, we get minus negative log likelihood is minus the sum.  Of the log of this plus the log of this. Okay. Any questions on that? That's just straightforward logs blah blah blah, okay.  No questions, okay.  So since in a neural network remodeling the mapping from x2t.  When there's no parameters in here of the model, this is what we're trying to model the probably the output given the input.  Right. That's what logistic regression does. Its Woodstock Max regression does.  So when we're minimizing this for their structure are parameters we can just drop this term.  Okay, that's too bright. Okay. So here's the idea. Here's a regression problem. We have all these blue dots. That's the data.  We've got we're trying to fit the data. But let's let's assume right now.  Did this red line is the underlying deterministic function that generated this data? How did it generate this data by drawing from a gaussian with 0 mean at the value of the of the  Red curve, so all these points were drawn from a gaussian year and now she in here guessing here guessing here etcetera. So we assume this so this is like noise. OK it could be measurement noise. It could be actual noise if you're in a bar.  Okay. So this is the this thing represents the probably the target cuz it's Galaxy and distributed x 0.  Given W and betta, I don't remember what beta is or what we're talking about here.  But it's just I think it's just some picture I found on the internet and I wanted to use it because it makes the point well.  So another words to probably the target given the data is his calcium.  Okay, so here's the target.  And HVAC Savannah is our model of the data.  So this is our neural network regression thing etcetera.  So if we make this assumption that when were trying to do regression the data has gassy and 0 Min gassy and noise. Then the probability of any one of these targets are given where it is. It has to be where it is because it's around this red line.  Yes, actually so h of X event is we assume that's the the equation for the red line sorry, that was a little  I want a little too far ahead. So this is the Red Line This is the deterministic function. And this is the probability of sum Target given there.  So there's some deterministic function with some zero mean additive Galaxy noise. So a Target is HVAC Savannah at that point plus Epsilon for Epsilon is the gaussian noise.  well  That means Epsilon equals T minus age of action pen to subtracting from both sides.  So the likelihood of the data looks like this product of a gaussian.  And now instead of HVAC Savannah in there. I put my neural-net cuz that's what I want to match. I want to match that underlying deterministic function in spite of the noise.  Okay.  And this notation here why of excavation semicolon W emphasizes that our model here is parameterize by Debbie.  Pretty standard notation of a little semicolon and then the parameters.  Okay, the negative log likelihood of this is this.  Right here is negative log just in front of all of this. I haven't changed anything here. Nothing behind the curtain.  And now I move the login.  not login logout login I get this  Okay, so if you take the log of e to something you get this something.  And the something is there for every data point so I can pull it out in front.  So when I take the log of this I get this it's a song because I took the log until I get that part. Okay, and what is that?  That looks a lot like some squared error scaled by this number, which doesn't have anything to do with it mean.  What's this other thing?  That's the constant out front came it became a since it was negative log likelihood. It came became a plus but we're not trying to fit that.  So we can remove that term because it's constant with respect to the weights cuz we're not trying to fit Sigma. We're trying to fit this guy.  And this Factor doesn't do anything either. It's just me and we can assume the signals or some constant.  and  that doesn't affect the minimum. It's still going to be the same minimum whether this is here or not and I get  Do-do-do-do do-do-do-do.  sum squared error  So do do do do do-do-do-do is the sound from Twilight Zone, and I'm going to say it a lot.  so  to recap  when doing regression if we assume the targets are ghosting distributed.  And we maximize the likelihood of the data by minimizing the negative log likelihood we get some screws are.  Okay, pretty cool. Huh? Any questions? Yeah.  When was this discovered?  probably by gas or something like that so old  this is if you're doing regression, that's not logistic regression.  This is just fitting data not trying to categorize stay there. We'll get to that. Don't worry coming.  So how does this lead to cross it?  for logistic regression  okay, so logistic regression should output the probability of category 1  so we want the network to produce the probability that the input is in category one. That is the output.  should be the probability category 1 given X  So that's again basically categorization.  So  How can so that that's like a coin flip, right?  And okay, so what's the likelihood of the data now?  Okay, is it a Galaxy in?  That doesn't make any sense here. Right? We don't think you know, this is Happy plus or minus some gassy and noise, right? It doesn't make sense. It's not the right distribution of the data right? Not the right likelihood form. So what do you think it is worth what distribution models are coin flip?  Bernoulli  Okay.  And there it is. Okay. So again  We assume that the likely it is the product of all the data points.  Which is and this is the distribution.  Why haven't the tea event 1-5 end in one month? So what if it why is in category one? So what's the probability that sin category 1 it's one because the 10 category one. Okay, so  Reuse tea event equals one category one that makes y ven to the 1 power which is just y ven. So that's what our network is saying is the output 1 - y ven * 1 - but wait even is one this becomes a zero. And so this goes away cuz it's this to the zeroth power. So we just end up with Y ven.  What if it's in Category 2?  What's the probability of it being in category too? Well, that that would if it has a zero Target, then this becomes Iran this becomes one and we get this was 1-0. So this is the probability that 10 Category 2 that becomes one that becomes one. We just send up with that.  Okay makes sense.  That's pretty cool a t kind of gates those two things.  Okay. So now this is the likelihood of the data.  He believed me so far.  So what we're trying to do is first we have to have some probability distribution over the data.  Last four linear or just plain regressing we assumed that the distribution was gassy and because we assumed there was some underlying deterministic function to which we had a gaussian noise here because it's a coin flip. We use a Bernoulli distribution for the likelihood.  Okay.  So what's the negative log likelihood of that?  Well negative log of this is remove the log inside at the sum. He had log of that to the T and you get  Wow, there it is cross entropy.  Okay, so to recap.  When we're doing two category classification if we assume the targets are Bernoulli distribution and maximize the likelihood of data by minimizing the negative log likelihood. We find that we need to minimize cross entropy error.  What's he doesn't play?  so  I'm in 8 minutes. How does this lead to cross and for multinomial regression?  Okay. Now we have more than 2 outputs.  Let's assume they're see outputs one for each category now he want.  I lit the probably is that the case output is the probability that that input is in category k?  And we want T soup and k equal to one is an injury or otherwise, so that's one hot encoding.  How do we write the likelihood?  Okay. Well we can just put it up here the TV up here, right? Why is output which to probably category K if category this is category K and it is category K that's going to be a one if it's not category K. It's going to be a zero. So all the other categories turn to zero in the end up with just the guy for that category.  Okay, so that's the probably one pattern.  It's the product over all these wise.  And most of them will turn to one with a one-out and coding except for the one that has the one so that's the probability distribution. That's a probably one pattern.  So, you know, this is just illustrating my point. Okay this to the zero that to the zero, that's the one that 2-0.  okay, just  any questions  Okay.  So we can write the entire likelihood has the product of the probabilities of all the data points. So now we have this and this and this and this should start to look kind of familiar. So we take the negative log of that forget a song and some and this  Do-do-do-do do-do-do-do. So we get we get processed for multiple classes.  so again  enduring Seaway classification multiple multinomial regression if we assume the targets are multinomial a distributed processing.  Okay, so in the next lecture, okay, so I told you all this in the next lecture. I'm going to talk about some other approaches to objective functions. These aren't the only ones.  And then start with some tricks of the trade, which is based on an older paper by young laocoon, but some of them are relevant and don't make sense.  Yeah, actually I have nobody can solve my problem creasing instead of decreasing. Yeah increasing. I mean, yeah, it hurts my training to test it for two rows gradually decreasing 0 and then I asked her if she calls me to figure it out.  Maybe you're learning rate is too high and weight.  Airplanes lyrics  smaller number increase laptop, but still interested.  Okay, so nobody can find out which one so, where is this where you're changing the weights? Yeah, so why it's got the label label - 1/4 Mi South product key for this and why for that? Yeah, okay.  Tell me how I am just seems cracked.  I'm sure this is hard for me to read cuz these are all different things in my operation.  So you could Logistics.  Do something again?  To hear you're doing what it is at a Timber at a 1/2 as a woman x o what's encode Logistics base is in a then it is what it is or why they need to get their own. Okay. Okay. Okay 13 live doppler.  Do you have you tried resetting the validation last is 0 right? Yeah, you can do a private post to us and show us your code and maybe somebody will find the answer. I don't see anything wrong with that 5 p.m.  Yeah, I don't know I can find out when you're using the way to just trained to do the lost right now too sure of that. Okay, I would you know, I often like print things out to see make sure they're what I think they are. You know, if you have tried that I see what's going on.  Crab Shack  I need a way to get updated and has lost all the secrecy. Can you just look at what the output of the network is for your test examples? It is like just something easy reminder right but  does it make sense at all? I mean sometimes  finishing the war all the time.  When I put all the time.  Tell me all the time or the place is always the same thing for every input. Did you remember the Z score your test set?  Okay.  Are you sure you're reading in the test set properly or the holdouts it properly?  UC San Diego podcast "
}